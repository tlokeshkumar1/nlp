{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPNO0ZfwYAB/VvL7B0R8b7k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tlokeshkumar1/nlp/blob/master/one.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3Ih3V5WT5m6"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from retrofitting import Retrofitting\n",
        "\n",
        "# Define the sets of tweets and labels\n",
        "X = pd.concat([pd.read_csv('D11.csv'), pd.read_csv('D2.csv')])['hashtags'].values\n",
        "Advice = [\"['Stay at home']\",\"['wash hands']\",\"['wear mask']\",\"['social distancing']\"]\n",
        "China = [\"['Wuhan']\",\"['China Coronavirus Updates']\",\"['China news']\",\"['other tweets related to China']\"]\n",
        "Mask = [\"['Mask shortage']\",\"['wear mask']\",\"['mask types']\",\"['N50']\",\"['N95']\",\"['3M8210']\",\"['3M9001']\",\"['3M9322']\",\"['3M9501']\"]\n",
        "News = [\"['Coronavirus updates']\",\"['news']\",\"['rules']\"]\n",
        "Transportation = [\"['Flights']\",\"['traffic']\",\"['traveling']\"]\n",
        "USA = [\"['U.S. Coronavirus Updates']\",\"['COVID19']\",\"['U.S. news']\",\"['United States']\",\"['US']\",\"['USA']\"]\n",
        "Vaccine = [\"['Vaccine news']\",\"['vaccine progress']\",\"['vaccine injection']\"]\n",
        "L = Advice + China + Mask + News + Transportation + USA + Vaccine\n",
        "\n",
        "# Initialize a pre-trained BERT model\n",
        "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "\n",
        "# Embed the sentences and labels\n",
        "X_emb = model.encode(X)\n",
        "L_emb = model.encode(L)\n",
        "\n",
        "# Load the external knowledge graph (ConceptNet)\n",
        "cn_data = pd.read_csv('conceptnet.csv', header=None, sep='\\t', names=['start', 'relation', 'end'])\n",
        "\n",
        "# Create the retrofitting model\n",
        "retro = Retrofitting(model=model, cn_data=cn_data, l2_normalize=True, use_all_singular_values=True)\n",
        "\n",
        "# Retrfoit the sentence and label embeddings to the knowledge graph space\n",
        "X_emb_kg = retro.fit_transform(X_emb)\n",
        "L_emb_kg = retro.fit_transform(L_emb)\n",
        "\n",
        "# Define the labels for the tweets\n",
        "labels = np.array(['Vaccine', 'USA', 'Transportation', 'News', 'Mask', 'China', 'Advice'])\n",
        "\n",
        "# Classify the tweets based on the cosine similarity between embeddings\n",
        "predictions = []\n",
        "for x in X_emb_kg:\n",
        "    cosine_sim = np.dot(x, L_emb_kg.T)\n",
        "    prediction = labels[np.argmax(cosine_sim)]\n",
        "    predictions.append(prediction)\n",
        "\n",
        "# Print the predicted labels\n",
        "print(predictions)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Define the tweet categories\n",
        "Advice = [\"['Stay at home']\",\"['wash hands']\",\"['wear mask']\",\"['social distancing']\"]\n",
        "China = [\"['Wuhan']\",\"['China Coronavirus Updates']\",\"['China news']\",\"['other tweets related to China']\"]\n",
        "Mask = [\"['Mask shortage']\",\"['wear mask']\",\"['mask types']\",\"['N50']\",\"['N95']\",\"['3M8210']\",\"['3M9001']\",\"['3M9322']\",\"['3M9501']\"]\n",
        "News = [\"['Coronavirus updates']\",\"['news']\",\"['rules']\"]\n",
        "Transportation = [\"['Flights']\",\"['traffic']\",\"['traveling']\"]\n",
        "USA = [\"['U.S. Coronavirus Updates']\",\"['COVID19']\",\"['U.S. news']\",\"['United States']\",\"['US']\",\"['USA']\"]\n",
        "Vaccine = [\"['Vaccine news']\",\"['vaccine progress']\",\"['vaccine injection']\"]\n",
        "\n",
        "# Create a dataframe with all the tweets\n",
        "tweets_df = pd.read_csv('tweets1.csv').append(pd.read_csv('tweets2.csv'))\n",
        "tweets_df.reset_index(inplace=True, drop=True)\n",
        "\n",
        "# Extract the tweet text from the hashtags column\n",
        "tweets_df['text'] = tweets_df['hashtags'].str.extract(r\"'(.*?)'\").fillna('')\n",
        "\n",
        "# Define the S-BERT model\n",
        "sbert_model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "\n",
        "# Compute embeddings for all the tweets\n",
        "tweet_embeddings = sbert_model.encode(tweets_df['text'])\n",
        "\n",
        "# Compute embeddings for all the tweet categories\n",
        "category_embeddings = sbert_model.encode(Advice + China + Mask + News + Transportation + USA + Vaccine)\n",
        "\n",
        "# Compute the cosine similarity between each tweet embedding and each category embedding\n",
        "similarity_matrix = cosine_similarity(tweet_embeddings, category_embeddings)\n",
        "\n",
        "# Assign each tweet to the category with the highest cosine similarity\n",
        "predicted_labels = []\n",
        "for i in range(len(tweets_df)):\n",
        "    predicted_label_index = similarity_matrix[i].argmax()\n",
        "    predicted_label = ['Vaccine','USA','Transportation','News','Mask','China','Advice'][predicted_label_index]\n",
        "    predicted_labels.append(predicted_label)\n",
        "\n",
        "# Add the predicted labels to the dataframe\n",
        "tweets_df['predicted_label'] = predicted_labels\n"
      ],
      "metadata": {
        "id": "ZejkismvUC9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "from retrofitting import Retrofitter\n",
        "\n",
        "# Load pretrained word embeddings (e.g., GloVe)\n",
        "word_vectors = np.load('glove_word_vectors.npy')\n",
        "word_vocab = np.load('glove_word_vocab.npy')\n",
        "\n",
        "# Load ConceptNet edges\n",
        "edges = []\n",
        "with open('conceptnet_edges.txt', 'r') as f:\n",
        "    for line in f:\n",
        "        line = line.strip().split('\\t')\n",
        "        edges.append((line[1], line[0], float(line[2])))  # reverse edge direction for retrofitting\n",
        "\n",
        "# Convert edges to sparse matrix\n",
        "num_words = len(word_vocab)\n",
        "edge_indices = {}\n",
        "edge_data = []\n",
        "for i, (u, v, w) in enumerate(edges):\n",
        "    if u in word_vocab and v in word_vocab:\n",
        "        j = word_vocab.index(u)\n",
        "        k = word_vocab.index(v)\n",
        "        if j != k:\n",
        "            if (j, k) not in edge_indices:\n",
        "                edge_indices[(j, k)] = len(edge_indices)\n",
        "                edge_data.append(0.0)\n",
        "            edge_data[edge_indices[(j, k)]] += w\n",
        "edge_indices = np.array(list(edge_indices.keys())).T\n",
        "edge_data = np.array(edge_data)\n",
        "edge_matrix = sp.csr_matrix((edge_data, edge_indices), shape=(num_words, num_words))\n",
        "\n",
        "# Retrofit word embeddings with ConceptNet\n",
        "retrofitter = Retrofitter(word_vectors, edge_matrix)\n",
        "retrofitted_vectors = retrofitter.fit(alpha=0.5, beta=0.5, num_iters=10)\n",
        "\n",
        "# Save retrofitted word embeddings\n",
        "np.save('retrofitted_word_vectors.npy', retrofitted_vectors)\n"
      ],
      "metadata": {
        "id": "KkvTP2yqUIFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Initialize S-BERT model and GloVe embeddings\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/bert-base-nli-mean-tokens\")\n",
        "model = AutoModel.from_pretrained(\"sentence-transformers/bert-base-nli-mean-tokens\")\n",
        "glove_embeddings = {}\n",
        "\n",
        "# Load GloVe embeddings\n",
        "with open('glove.6B.300d.txt', 'r', encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        glove_embeddings[word] = coefs\n",
        "\n",
        "# Define knowledge graph embedding function\n",
        "def get_knowledge_graph_embeddings(vocab):\n",
        "    knowledge_graph = {}\n",
        "    with open('conceptnet_embeddings.txt', 'r', encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            if word in vocab:\n",
        "                coefs = np.asarray(values[1:], dtype='float32')\n",
        "                knowledge_graph[word] = coefs\n",
        "    return knowledge_graph\n",
        "\n",
        "# Define function to get embeddings for a list of words\n",
        "def get_embeddings(words):\n",
        "    embeddings = []\n",
        "    for word in words:\n",
        "        if word in glove_embeddings:\n",
        "            embeddings.append(glove_embeddings[word])\n",
        "        else:\n",
        "            embeddings.append(np.zeros(300)) # use 0 vectors for out-of-vocabulary words\n",
        "    return torch.tensor(embeddings)\n",
        "\n",
        "# Define function to learn projection matrix\n",
        "def learn_projection_matrix(vocab):\n",
        "    # Get knowledge graph embeddings for vocab\n",
        "    QV = get_knowledge_graph_embeddings(vocab)\n",
        "    # Get S-BERT embeddings for vocab\n",
        "    input_ids = tokenizer(list(vocab), return_tensors=\"pt\", padding=True).input_ids\n",
        "    fV = model(input_ids).pooler_output.detach().numpy()\n",
        "    # Learn projection matrix\n",
        "    reg = LinearRegression().fit(fV, list(QV.values()))\n",
        "    P = reg.coef_.T\n",
        "    return P\n",
        "\n",
        "# Define function to get embeddings for a list of labels\n",
        "def get_label_embeddings(labels, P):\n",
        "    # Get S-BERT embeddings for labels\n",
        "    input_ids = tokenizer(list(labels), return_tensors=\"pt\", padding=True).input_ids\n",
        "    f_labels = model(input_ids).pooler_output.detach().numpy()\n",
        "    # Project S-BERT embeddings into knowledge graph embedding space\n",
        "    f_labels = np.dot(f_labels, P)\n",
        "    return f_labels\n",
        "\n",
        "# Define function to predict labels for a list of tweets\n",
        "def predict_labels(tweets, labels, P):\n",
        "    # Get S-BERT embeddings for tweets\n",
        "    input_ids = tokenizer(list(tweets), return_tensors=\"pt\", padding=True).input_ids\n",
        "    f_tweets = model(input_ids).pooler_output.detach().numpy()\n",
        "    # Project S-BERT embeddings into knowledge graph embedding space\n",
        "    f_tweets = np.dot(f_tweets, P)\n",
        "    # Get label embeddings and calculate cosine similarity with tweets\n",
        "    f_labels = get_label_embeddings(labels, P)\n",
        "    cos_sim = np.dot(f_tweets, f_labels.T) / (np.linalg.norm(f_tweets, axis=1)[:, np.newaxis] * np.linalg.norm(f_labels, axis=1))\n",
        "    # Get predicted labels\n",
        "    predicted_labels = [labels[i] for i in np.argmax(cos_sim, axis=1)]\n",
        "    return predicted_labels\n"
      ],
      "metadata": {
        "id": "NIyHjYHZUqlE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}