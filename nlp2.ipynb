{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tlokeshkumar1/nlp/blob/master/nlp2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrBaHfByscvb",
        "outputId": "79827227-cc7e-419e-cfa3-efce8d620e10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (1.21.6)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (2022.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQe4-4W9skyW",
        "outputId": "645d524b-5b78-4d93-b032-811cbc00a435"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 895
        },
        "id": "QtL1aU_xsOEj",
        "outputId": "f5702f15-9295-41fd-bc57-78f0f67403b2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                           user_name               user_location  \\\n",
              "0                             ·èâ·é•‚òª’¨ÍÇÖœÆ                  astroworld   \n",
              "1                      Tom Basile üá∫üá∏                New York, NY   \n",
              "2                    Time4fisticuffs            Pewee Valley, KY   \n",
              "3                        ethel mertz        Stuck in the Middle    \n",
              "4                           DIPR-J&K           Jammu and Kashmir   \n",
              "5                   üéπ Franz Schubert                 –ù–æ–≤–æ—Ä–æÃÅ—Å—Å–∏—è   \n",
              "6                       hr bartender             Gainesville, FL   \n",
              "7                     Derbyshire LPC                         NaN   \n",
              "8                  Prathamesh Bendre                         NaN   \n",
              "9  Member of Christ üá®üá≥üá∫üá∏üáÆüá≥üáÆüá©üáßüá∑üá≥üá¨üáßüá©üá∑üá∫  üëáüèªlocation at link belowüëáüèª   \n",
              "\n",
              "                                    user_description         user_created  \\\n",
              "0  wednesday addams as a disney princess keepin i...  2017-05-26 05:46:42   \n",
              "1  Husband, Father, Columnist & Commentator. Auth...  2009-04-16 20:06:23   \n",
              "2  #Christian #Catholic #Conservative #Reagan #Re...  2009-02-28 18:57:41   \n",
              "3  #Browns #Indians #ClevelandProud #[]_[] #Cavs ...  2019-03-07 01:45:06   \n",
              "4  üñäÔ∏èOfficial Twitter handle of Department of Inf...  2017-02-12 06:45:15   \n",
              "5  üéº  #–ù–æ–≤–æ—Ä–æÃÅ—Å—Å–∏—è #Novorossiya #–æ—Å—Ç–∞–≤–∞–π—Å—è–¥–æ–º–∞ #S...  2018-03-19 16:29:52   \n",
              "6  Workplace tips and advice served up in a frien...  2008-08-12 18:19:49   \n",
              "7                                                NaN  2012-02-03 18:08:10   \n",
              "8   A poet, reiki practitioner and a student of law.  2015-04-25 08:15:41   \n",
              "9  Just as the body is one & has many members, & ...  2014-08-17 04:53:22   \n",
              "\n",
              "   user_followers  user_friends  user_favourites  user_verified  \\\n",
              "0             624           950            18775          False   \n",
              "1            2253          1677               24           True   \n",
              "2            9275          9525             7254          False   \n",
              "3             197           987             1488          False   \n",
              "4          101009           168              101          False   \n",
              "5            1180          1071             1287          False   \n",
              "6           79956         54810             3801          False   \n",
              "7             608           355               95          False   \n",
              "8              25            29               18          False   \n",
              "9           55201         34239            29802          False   \n",
              "\n",
              "                  date                                               text  \\\n",
              "0  2020-07-25 12:27:21  If I smelled the scent of hand sanitizers toda...   \n",
              "1  2020-07-25 12:27:17  Hey @Yankees @YankeesPR and @MLB - wouldn't it...   \n",
              "2  2020-07-25 12:27:14  @diane3443 @wdunlap @realDonaldTrump Trump nev...   \n",
              "3  2020-07-25 12:27:10  @brookbanktv The one gift #COVID19 has give me...   \n",
              "4  2020-07-25 12:27:08  25 July : Media Bulletin on Novel #CoronaVirus...   \n",
              "5  2020-07-25 12:27:06  #coronavirus #covid19 deaths continue to rise....   \n",
              "6  2020-07-25 12:27:03  How #COVID19 Will Change Work in General (and ...   \n",
              "7  2020-07-25 12:27:00  You now have to wear face coverings when out s...   \n",
              "8  2020-07-25 12:26:59  Praying for good health and recovery of @Chouh...   \n",
              "9  2020-07-25 12:26:54  POPE AS GOD - Prophet Sadhu Sundar Selvaraj. W...   \n",
              "\n",
              "                            hashtags               source  is_retweet  \n",
              "0                                NaN   Twitter for iPhone       False  \n",
              "1                                NaN  Twitter for Android       False  \n",
              "2                        ['COVID19']  Twitter for Android       False  \n",
              "3                        ['COVID19']   Twitter for iPhone       False  \n",
              "4  ['CoronaVirusUpdates', 'COVID19']  Twitter for Android       False  \n",
              "5         ['coronavirus', 'covid19']      Twitter Web App       False  \n",
              "6          ['COVID19', 'Recruiting']               Buffer       False  \n",
              "7                                NaN            TweetDeck       False  \n",
              "8       ['covid19', 'covidPositive']  Twitter for Android       False  \n",
              "9      ['HurricaneHanna', 'COVID19']   Twitter for iPhone       False  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-dbbbc18d-b59a-4d16-bd82-b84a03913882\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_name</th>\n",
              "      <th>user_location</th>\n",
              "      <th>user_description</th>\n",
              "      <th>user_created</th>\n",
              "      <th>user_followers</th>\n",
              "      <th>user_friends</th>\n",
              "      <th>user_favourites</th>\n",
              "      <th>user_verified</th>\n",
              "      <th>date</th>\n",
              "      <th>text</th>\n",
              "      <th>hashtags</th>\n",
              "      <th>source</th>\n",
              "      <th>is_retweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>·èâ·é•‚òª’¨ÍÇÖœÆ</td>\n",
              "      <td>astroworld</td>\n",
              "      <td>wednesday addams as a disney princess keepin i...</td>\n",
              "      <td>2017-05-26 05:46:42</td>\n",
              "      <td>624</td>\n",
              "      <td>950</td>\n",
              "      <td>18775</td>\n",
              "      <td>False</td>\n",
              "      <td>2020-07-25 12:27:21</td>\n",
              "      <td>If I smelled the scent of hand sanitizers toda...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Twitter for iPhone</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Tom Basile üá∫üá∏</td>\n",
              "      <td>New York, NY</td>\n",
              "      <td>Husband, Father, Columnist &amp; Commentator. Auth...</td>\n",
              "      <td>2009-04-16 20:06:23</td>\n",
              "      <td>2253</td>\n",
              "      <td>1677</td>\n",
              "      <td>24</td>\n",
              "      <td>True</td>\n",
              "      <td>2020-07-25 12:27:17</td>\n",
              "      <td>Hey @Yankees @YankeesPR and @MLB - wouldn't it...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Twitter for Android</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Time4fisticuffs</td>\n",
              "      <td>Pewee Valley, KY</td>\n",
              "      <td>#Christian #Catholic #Conservative #Reagan #Re...</td>\n",
              "      <td>2009-02-28 18:57:41</td>\n",
              "      <td>9275</td>\n",
              "      <td>9525</td>\n",
              "      <td>7254</td>\n",
              "      <td>False</td>\n",
              "      <td>2020-07-25 12:27:14</td>\n",
              "      <td>@diane3443 @wdunlap @realDonaldTrump Trump nev...</td>\n",
              "      <td>['COVID19']</td>\n",
              "      <td>Twitter for Android</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ethel mertz</td>\n",
              "      <td>Stuck in the Middle</td>\n",
              "      <td>#Browns #Indians #ClevelandProud #[]_[] #Cavs ...</td>\n",
              "      <td>2019-03-07 01:45:06</td>\n",
              "      <td>197</td>\n",
              "      <td>987</td>\n",
              "      <td>1488</td>\n",
              "      <td>False</td>\n",
              "      <td>2020-07-25 12:27:10</td>\n",
              "      <td>@brookbanktv The one gift #COVID19 has give me...</td>\n",
              "      <td>['COVID19']</td>\n",
              "      <td>Twitter for iPhone</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>DIPR-J&amp;K</td>\n",
              "      <td>Jammu and Kashmir</td>\n",
              "      <td>üñäÔ∏èOfficial Twitter handle of Department of Inf...</td>\n",
              "      <td>2017-02-12 06:45:15</td>\n",
              "      <td>101009</td>\n",
              "      <td>168</td>\n",
              "      <td>101</td>\n",
              "      <td>False</td>\n",
              "      <td>2020-07-25 12:27:08</td>\n",
              "      <td>25 July : Media Bulletin on Novel #CoronaVirus...</td>\n",
              "      <td>['CoronaVirusUpdates', 'COVID19']</td>\n",
              "      <td>Twitter for Android</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>üéπ Franz Schubert</td>\n",
              "      <td>–ù–æ–≤–æ—Ä–æÃÅ—Å—Å–∏—è</td>\n",
              "      <td>üéº  #–ù–æ–≤–æ—Ä–æÃÅ—Å—Å–∏—è #Novorossiya #–æ—Å—Ç–∞–≤–∞–π—Å—è–¥–æ–º–∞ #S...</td>\n",
              "      <td>2018-03-19 16:29:52</td>\n",
              "      <td>1180</td>\n",
              "      <td>1071</td>\n",
              "      <td>1287</td>\n",
              "      <td>False</td>\n",
              "      <td>2020-07-25 12:27:06</td>\n",
              "      <td>#coronavirus #covid19 deaths continue to rise....</td>\n",
              "      <td>['coronavirus', 'covid19']</td>\n",
              "      <td>Twitter Web App</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>hr bartender</td>\n",
              "      <td>Gainesville, FL</td>\n",
              "      <td>Workplace tips and advice served up in a frien...</td>\n",
              "      <td>2008-08-12 18:19:49</td>\n",
              "      <td>79956</td>\n",
              "      <td>54810</td>\n",
              "      <td>3801</td>\n",
              "      <td>False</td>\n",
              "      <td>2020-07-25 12:27:03</td>\n",
              "      <td>How #COVID19 Will Change Work in General (and ...</td>\n",
              "      <td>['COVID19', 'Recruiting']</td>\n",
              "      <td>Buffer</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Derbyshire LPC</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2012-02-03 18:08:10</td>\n",
              "      <td>608</td>\n",
              "      <td>355</td>\n",
              "      <td>95</td>\n",
              "      <td>False</td>\n",
              "      <td>2020-07-25 12:27:00</td>\n",
              "      <td>You now have to wear face coverings when out s...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>TweetDeck</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Prathamesh Bendre</td>\n",
              "      <td>NaN</td>\n",
              "      <td>A poet, reiki practitioner and a student of law.</td>\n",
              "      <td>2015-04-25 08:15:41</td>\n",
              "      <td>25</td>\n",
              "      <td>29</td>\n",
              "      <td>18</td>\n",
              "      <td>False</td>\n",
              "      <td>2020-07-25 12:26:59</td>\n",
              "      <td>Praying for good health and recovery of @Chouh...</td>\n",
              "      <td>['covid19', 'covidPositive']</td>\n",
              "      <td>Twitter for Android</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Member of Christ üá®üá≥üá∫üá∏üáÆüá≥üáÆüá©üáßüá∑üá≥üá¨üáßüá©üá∑üá∫</td>\n",
              "      <td>üëáüèªlocation at link belowüëáüèª</td>\n",
              "      <td>Just as the body is one &amp; has many members, &amp; ...</td>\n",
              "      <td>2014-08-17 04:53:22</td>\n",
              "      <td>55201</td>\n",
              "      <td>34239</td>\n",
              "      <td>29802</td>\n",
              "      <td>False</td>\n",
              "      <td>2020-07-25 12:26:54</td>\n",
              "      <td>POPE AS GOD - Prophet Sadhu Sundar Selvaraj. W...</td>\n",
              "      <td>['HurricaneHanna', 'COVID19']</td>\n",
              "      <td>Twitter for iPhone</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dbbbc18d-b59a-4d16-bd82-b84a03913882')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-dbbbc18d-b59a-4d16-bd82-b84a03913882 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-dbbbc18d-b59a-4d16-bd82-b84a03913882');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import pandas as pd\n",
        "RESULTS_DIR = 'drive/MyDrive'\n",
        "covid19_tweets = pd.read_csv(RESULTS_DIR +'/covid19_tweets.csv')\n",
        "covid19_tweets.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAZ8wtD-tGaU",
        "outputId": "5719616b-d39e-41b6-ab72-de1b1a12046d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import string\n",
        "from pathlib import Path\n",
        "RESULTS_DIR = 'drive/MyDrive'\n",
        "covid19_tweets = pd.read_csv(RESULTS_DIR +'/covid19_tweets.csv')\n",
        "\n",
        "\n",
        "# Concatenate the papers from different sources into one corpus dataframe.\n",
        "corpus_df = pd.concat([covid19_tweets])\n",
        "\n",
        "def clean_sentence(sentence):\n",
        "    \"\"\"\n",
        "    Cleans an individual sentence.\n",
        "    \n",
        "    Args:\n",
        "        sentence: An individual sentence from the corpus.\n",
        "    Returns:\n",
        "        clean_sentence: A cleaned sentence with unwanted linespaces removed.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Remove titles before linespaces\n",
        "    try:\n",
        "        clean_sentence = re.findall('\\n\\n(.*)', sentence)[-1]\n",
        "    except IndexError:\n",
        "        clean_sentence = sentence\n",
        "    \n",
        "    return clean_sentence\n",
        "\n",
        "def clean_sentences(sentences):\n",
        "    \"\"\"\n",
        "    Cleans each sentence in a list of sentences, corresponding to one article.\n",
        "    \n",
        "    Args:\n",
        "        sentences: A list of sentences from an article (paper).\n",
        "    Returns:\n",
        "        cleaned_sentences_no_punctuation: A list of cleaned sentences, with \"sentences\" that only contain punctuation removed.\n",
        "    \"\"\"\n",
        "    \n",
        "    cleaned_sentences = [clean_sentence(sentence) for sentence in sentences]\n",
        "    \n",
        "    # Remove sentences with only punctuation\n",
        "    cleaned_sentences_no_punctuation = [sentence for sentence in cleaned_sentences if sentence not in string.punctuation]\n",
        "    \n",
        "    return cleaned_sentences_no_punctuation\n",
        "    \n",
        "    \n",
        "# Retain the paper ID throughout the analysis.\n",
        "# The paper ID can be re-used to refer back to the relevant paper.\n",
        "corpus = corpus_df[['user_name', 'text']].values\n",
        "corpus = [(user_name, document) for user_name, document in corpus if type(document)==str]\n",
        "\n",
        "# Split into sentences for each article in the corpus\n",
        "documents = [(user_name, nltk.sent_tokenize(document)) for user_name, document in corpus]\n",
        "\n",
        "# Clean each sentence\n",
        "documents_clean = [(user_name, clean_sentences(document)) for user_name, document in documents]\n",
        "        \n",
        "# Flatten documents into a list of sentences\n",
        "sentences_clean = [item for sublist in documents_clean for item in sublist]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bM-H4KgAtXru",
        "outputId": "fa14acd3-c662-43e4-e5e6-0f950c60ff32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 68 kB 4.3 MB/s \n",
            "\u001b[?25hCollecting pybind11>=2.2\n",
            "  Using cached pybind11-2.10.2-py3-none-any.whl (222 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from fasttext) (57.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from fasttext) (1.21.6)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp38-cp38-linux_x86_64.whl size=3133547 sha256=ab4c671098d6482336ff26cf01e13cd0ee8583c1bd86f9371c179ca58615b330\n",
            "  Stored in directory: /root/.cache/pip/wheels/93/61/2a/c54711a91c418ba06ba195b1d78ff24fcaad8592f2a694ac94\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.2 pybind11-2.10.2\n"
          ]
        }
      ],
      "source": [
        "pip install fasttext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIPWrx_0tSJ7",
        "outputId": "b2059004-7d00-467e-8a6e-89c31d0b896d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "quarantine\n",
            "{'shelter in place', 'shutdown.', 'lock-down', 'shut downs', 'lock downs', 'shelter-in-place', 'quarantined', 'quarantining', 'shutdown', 'evacuations', 'stay-home', 'locked-down', 'curfew', 'quarantines', 'stay at home', 'lockdown', 'lockdowns', 'shut-downs', 'locked down', 'shut down', 'lock-downs', 'post shutdown', 'lock down', 'quarentine', 'stay home', 'stay-at-home', 'shutdowns', 'self quarantine', 'shut-down', 'quarantine', 'post-shutdown', 'self-quarantine'}\n"
          ]
        }
      ],
      "source": [
        "# We will use FastText for pre-trained word-embeddings.\n",
        "# As training word embeddings from the CORD-19 corpus and finding words with similar embeddings will be computationally expensive.\n",
        "import fasttext\n",
        "from fasttext import util\n",
        "keywords = ['quarantine', 'stay-at-home', 'shelter-in-place', 'shutdown', 'lockdown']\n",
        "\n",
        "\n",
        "util.download_model('en', if_exists='ignore')  # English\n",
        "\n",
        "ft = fasttext.load_model('cc.en.300.bin')\n",
        "\n",
        "# Word embeddings can be used to return words that are close within the vector space\n",
        "print(keywords[0])\n",
        "ft.get_nearest_neighbors(keywords[0])\n",
        "\n",
        "keywords_extended = []\n",
        "for keyword in keywords:\n",
        "    keywords_extended.append(keyword)\n",
        "    \n",
        "    nearest_neighbours = [word.lower() for similarity, word in ft.get_nearest_neighbors(keyword)]\n",
        "    keywords_extended += nearest_neighbours\n",
        "    \n",
        "keywords_set = set(keywords_extended)\n",
        "\n",
        "# Manually remove irrelevant words\n",
        "keywords_set = keywords_set - {'active-shooter', 'boil-water', 'high-security', 'bio-security', 'shutdown.the', 'pre-evacuation', 'biosecurity', 'evacuate','evacuation','evacuations'\n",
        "                              'sahm','stay-at','stay-at-home-dad','stay-at-home-mom','stay-at-home-moms','stay-at-home-mother','stay-at-home-mum', 'sahm'}\n",
        "\n",
        "# As the keywords returned are only unigrams (with hyphens) we can also include ngrams which may be found in the corpus.\n",
        "keywords_with_spaces = set([word.replace('-', ' ') for word in list(keywords_set)])\n",
        "keywords_set = keywords_set.union(keywords_with_spaces)\n",
        "\n",
        "print(keywords_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4-Zi6trtstZ",
        "outputId": "1f00e574-f8d1-419c-ff45-675ff59b7bcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 85 kB 3.4 MB/s \n",
            "\u001b[?25hCollecting transformers<5.0.0,>=4.6.0\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5.8 MB 14.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (4.64.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (1.13.0+cu116)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (0.14.0+cu116)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (1.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (1.7.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (3.7)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.3 MB 45.6 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub>=0.4.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 182 kB 54.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers) (3.0.9)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.6.2)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7.6 MB 51.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.12.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125938 sha256=c5003e80673958c7cc52e7e9a978ad83a03f0f33aabd0db37a6f757709af47c2\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/6f/8c/d88aec621f3f542d26fac0342bef5e693335d125f4e54aeffe\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers, sentencepiece, sentence-transformers\n",
            "Successfully installed huggingface-hub-0.11.1 sentence-transformers-2.2.2 sentencepiece-0.1.97 tokenizers-0.13.2 transformers-4.25.1\n"
          ]
        }
      ],
      "source": [
        "pip install -U sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUWYcouduCsU",
        "outputId": "a98c45d0-a6f1-4997-81c0-279dcfd4b160"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (1.0.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.2.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.7.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.21.6)\n"
          ]
        }
      ],
      "source": [
        "pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iisn5M0QuGpZ",
        "outputId": "189ee66e-0535-46f7-9fc5-1172e59fc38d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.8.0-py3-none-any.whl (452 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 452 kB 6.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.25.1)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 132 kB 52.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (4.64.1)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 212 kB 49.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets) (3.8.3)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (2022.11.0)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from datasets) (1.21.6)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.11.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: dill<0.3.7 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (22.1.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.8.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (6.0.3)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (2.1.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.8.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->datasets) (3.0.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 127 kB 35.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2022.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: urllib3, xxhash, responses, multiprocess, datasets\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed datasets-2.8.0 multiprocess-0.70.14 responses-0.18.0 urllib3-1.25.11 xxhash-3.1.0\n"
          ]
        }
      ],
      "source": [
        "pip install datasets transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install BERT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0dOHuB7KApx",
        "outputId": "b6a6920e-66c2-4053-dbdd-f7b030c8d789"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting BERT\n",
            "  Downloading bert-2.2.0.tar.gz (3.5 kB)\n",
            "Collecting erlastic\n",
            "  Downloading erlastic-2.0.0.tar.gz (6.8 kB)\n",
            "Building wheels for collected packages: BERT, erlastic\n",
            "  Building wheel for BERT (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for BERT: filename=bert-2.2.0-py3-none-any.whl size=3764 sha256=adc3add890788e6561eae8da11ac8c5e7f52bee40d557ef5913b3387c33071c7\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/11/40/6439aef2635f7f0137a79c4defb4c4e65dd051ec0198429e3b\n",
            "  Building wheel for erlastic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for erlastic: filename=erlastic-2.0.0-py3-none-any.whl size=6795 sha256=fe965f850a010eabc2bdf8228edd5ad4208922b4a1d904ce89dace7542e53c38\n",
            "  Stored in directory: /root/.cache/pip/wheels/ee/c9/a6/41a81618e939b746a3151700565d191bca832b6c345ea9b87a\n",
            "Successfully built BERT erlastic\n",
            "Installing collected packages: erlastic, BERT\n",
            "Successfully installed BERT-2.2.0 erlastic-2.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "id": "0GrN7MVtt0et",
        "outputId": "d83efc7e-e11d-46dd-b0c6-66004c0622ce"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-86790de25c2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'distilbert-base-nli-mean-tokens'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelevant_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'relevant_sentences' is not defined"
          ]
        }
      ],
      "source": [
        "# Using DistilBERT from Hugging Face for sentence embeddings\n",
        "# https://medium.com/huggingface/distilbert-8cf3380435b5\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
        "embeddings = model.encode(relevant_sentences, show_progress_bar=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzrEW61FuUZz",
        "outputId": "189bd81c-4887-4533-bddc-99578b50f541"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting umap-learn\n",
            "  Downloading umap-learn-0.5.3.tar.gz (88 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 88 kB 4.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from umap-learn) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.8/dist-packages (from umap-learn) (1.0.2)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.8/dist-packages (from umap-learn) (1.7.3)\n",
            "Requirement already satisfied: numba>=0.49 in /usr/local/lib/python3.8/dist-packages (from umap-learn) (0.56.4)\n",
            "Collecting pynndescent>=0.5\n",
            "  Downloading pynndescent-0.5.8.tar.gz (1.1 MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.1 MB 44.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from umap-learn) (4.64.1)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.8/dist-packages (from numba>=0.49->umap-learn) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from numba>=0.49->umap-learn) (57.4.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.8/dist-packages (from numba>=0.49->umap-learn) (5.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from pynndescent>=0.5->umap-learn) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.22->umap-learn) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata->numba>=0.49->umap-learn) (3.11.0)\n",
            "Building wheels for collected packages: umap-learn, pynndescent\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for umap-learn: filename=umap_learn-0.5.3-py3-none-any.whl size=82829 sha256=329490f46ed5f91d1955842f89be2a1b61a61182666cd19c904281ea2cd7fd9c\n",
            "  Stored in directory: /root/.cache/pip/wheels/a9/3a/67/06a8950e053725912e6a8c42c4a3a241410f6487b8402542ea\n",
            "  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pynndescent: filename=pynndescent-0.5.8-py3-none-any.whl size=55513 sha256=9b28dc6ecbedb23f19c034b72b548a9af34d4f1865a56729eff028f77782a42e\n",
            "  Stored in directory: /root/.cache/pip/wheels/1c/63/3a/29954bca1a27ba100ed8c27973a78cb71b43dc67aed62e80c3\n",
            "Successfully built umap-learn pynndescent\n",
            "Installing collected packages: pynndescent, umap-learn\n",
            "Successfully installed pynndescent-0.5.8 umap-learn-0.5.3\n"
          ]
        }
      ],
      "source": [
        "pip install umap-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-E2CAsmucVM",
        "outputId": "9ce9260e-0caf-4b59-c2d7-76bf219fc8a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: matplotlib-venn in /usr/local/lib/python3.8/dist-packages (0.11.7)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from matplotlib-venn) (3.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from matplotlib-venn) (1.7.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from matplotlib-venn) (1.21.6)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->matplotlib-venn) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->matplotlib-venn) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->matplotlib-venn) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->matplotlib-venn) (1.4.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib->matplotlib-venn) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "pip install matplotlib-venn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "id": "f_zcZ69huhI8",
        "outputId": "5b8aa1f8-9e1a-4328-f357-247047406b2d"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-e10025abc048>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m umap_embeddings = umap.UMAP(n_neighbors=15, \n\u001b[1;32m      4\u001b[0m                             \u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                             metric='cosine').fit_transform(embeddings)\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'embeddings' is not defined"
          ]
        }
      ],
      "source": [
        "import umap.umap_ as umap\n",
        "from keras.layers import Embedding\n",
        "umap_embeddings = umap.UMAP(n_neighbors=15, \n",
        "                            n_components=5, \n",
        "                            metric='cosine').fit_transform(embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1QOHxLKuxV9",
        "outputId": "35d6a190-ee7a-4194-d887-7b23d8087407"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting hdbscan\n",
            "  Downloading hdbscan-0.8.29.tar.gz (5.2 MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5.2 MB 6.7 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.8/dist-packages (from hdbscan) (1.0.2)\n",
            "Requirement already satisfied: cython>=0.27 in /usr/local/lib/python3.8/dist-packages (from hdbscan) (0.29.32)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.8/dist-packages (from hdbscan) (1.2.0)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.8/dist-packages (from hdbscan) (1.7.3)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from hdbscan) (1.21.6)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.20->hdbscan) (3.1.0)\n",
            "Building wheels for collected packages: hdbscan\n",
            "  Building wheel for hdbscan (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hdbscan: filename=hdbscan-0.8.29-cp38-cp38-linux_x86_64.whl size=2700857 sha256=09524eb878b869cb4abff23fab2938e32af087d44fe34c6aef81cf58036a1f75\n",
            "  Stored in directory: /root/.cache/pip/wheels/76/06/48/527e038689c581cc9e519c73840efdc7473805149e55bd7ffd\n",
            "Successfully built hdbscan\n",
            "Installing collected packages: hdbscan\n",
            "Successfully installed hdbscan-0.8.29\n"
          ]
        }
      ],
      "source": [
        "pip install hdbscan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "id": "BdbYa0YnvIOo",
        "outputId": "9051384d-ae47-4d3d-8555-b1dd8fbc1c35"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-8420de7a8c66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m cluster = hdbscan.HDBSCAN(min_cluster_size=20,\n\u001b[1;32m      4\u001b[0m                           \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'euclidean'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                           cluster_selection_method='eom').fit(umap_embeddings)\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'umap_embeddings' is not defined"
          ]
        }
      ],
      "source": [
        "import hdbscan\n",
        "\n",
        "cluster = hdbscan.HDBSCAN(min_cluster_size=20,\n",
        "                          metric='euclidean',                      \n",
        "                          cluster_selection_method='eom').fit(umap_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "id": "WRv8pbF9vKdf",
        "outputId": "3f2ed681-526f-4d8c-839d-7237c75ea837"
      },
      "outputs": [
        {
          "ename": "IndentationError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-6019dc3939a7>\"\u001b[0;36m, line \u001b[0;32m41\u001b[0m\n\u001b[0;31m    length = arr.shape[0]\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ],
      "source": [
        "# Names for each topic were manually created by looking through examples in each topic and the most relevant words\n",
        "topic_labels_dict = {\n",
        " -1: 'outliers',\n",
        "  0: 'electrical systems',\n",
        "  1: 'cellular shutdown',\n",
        "  2: 'australia',\n",
        "  3: 'korea',\n",
        "  4: 'links',\n",
        "  5: 'cities in lockdown',\n",
        "  6: 'canada',\n",
        "  7: 'historical quarantines',\n",
        "  8: 'sars',\n",
        "  9: 'thermoscanners',\n",
        "  10: 'animals',\n",
        "  11: 'copyright',\n",
        "  12: 'quarantine measures',\n",
        "  13: 'authorities',\n",
        "  14: 'japan',\n",
        "  15: 'studies on quarantined individuals',\n",
        "  16: 'lockdown & quarantine consequences',\n",
        "  17: 'increasing cases',\n",
        "  18: 'lockdown effect on transmission rate',\n",
        "  19: 'lockdown reducing infections',\n",
        "  20: 'quarantine length',\n",
        "  21: 'wuhan',\n",
        "  22: 'china',\n",
        "  23: 'early 2020',\n",
        "  24: 'quantative metrics',\n",
        "  25: 'other epidemics'\n",
        "}\n",
        "\n",
        "def get_centroid(arr):\n",
        "  \"\"\"\n",
        "  Retrieves the centroid of a cluster, given the set of coordinates of points within the cluster.\n",
        "  \n",
        "  Args:\n",
        "      arr: Array containing set of coordinates of points within a cluster.\n",
        "  Returns:\n",
        "      x_centre, y_centre: Coordinates of the centroid of the cluster.\n",
        "  \"\"\"\n",
        "    length = arr.shape[0]\n",
        "    sum_x = np.sum(arr[:, 0])\n",
        "    sum_y = np.sum(arr[:, 1])\n",
        "    x_centre, y_centre = sum_x/length, sum_y/length\n",
        "    return x_centre, y_centre\n",
        "  \n",
        "# Visualize clusters\n",
        "fig, ax = plt.subplots(figsize=(20, 10), dpi=200)\n",
        "outliers = result.loc[result.labels == -1, :]\n",
        "clustered = result.loc[result.labels != -1, :]\n",
        "plt.scatter(outliers.x, outliers.y, color='#BDBDBD', s=0.5)\n",
        "plt.scatter(clustered.x, clustered.y, c=clustered.labels, s=0.5, cmap='hsv_r')\n",
        "plt.colorbar()\n",
        "\n",
        "# Plot topic names\n",
        "for label in result['labels']:\n",
        "    # skip outliers\n",
        "    if label == -1:\n",
        "        pass\n",
        "    else:\n",
        "        label_name = topic_labels_dict[label]\n",
        "        coords = result[result['labels']==label][['x', 'y']].values\n",
        "        x_pos, y_pos = get_centroid(coords)\n",
        "        plt.text(x_pos, y_pos, label_name)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPCKdL5mwcuemTl6/8x+UOC",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}