{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM08GmNhCRsAR4qF2+URh3k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tlokeshkumar1/nlp/blob/master/second.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "bozyMaP3tOsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U sentence-transformers"
      ],
      "metadata": {
        "id": "n4-kZ-votS_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Python code for S-BERT-KG model for tweet classification\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "#Define function to obtain S-BERT embeddings for sentences\n",
        "def obtain_sbert_embeddings(sentences, model, tokenizer):\n",
        "# Tokenize the sentences\n",
        "tokens = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "# Obtain the S-BERT embeddings for the tokens\n",
        "with torch.no_grad():\n",
        "embeddings = model(tokens['input_ids'], attention_mask=tokens['attention_mask'])[0]\n",
        "return embeddings\n",
        "\n",
        "#Define function to learn a least-squares linear projection matrix\n",
        "def learn_projection_matrix(sbert_embeddings, kg_embeddings):\n",
        "# Convert the embeddings to numpy arrays\n",
        "sbert_embeddings = sbert_embeddings.detach().numpy()\n",
        "kg_embeddings = np.array([kg_embeddings[word] for word in sbert_embeddings])\n",
        "# Learn the projection matrix using linear regression\n",
        "reg = LinearRegression().fit(sbert_embeddings, kg_embeddings)\n",
        "projection_matrix = reg.coef_\n",
        "return projection_matrix\n",
        "\n",
        "#Define function for zero-shot text classification\n",
        "def zero_shot_classification(tweet_embeddings, label_embeddings, projection_matrix):\n",
        "# Project the tweet embeddings and label embeddings into the knowledge graph embedding space\n",
        "projected_tweet_embeddings = np.dot(tweet_embeddings, projection_matrix)\n",
        "projected_label_embeddings = np.dot(label_embeddings, projection_matrix)\n",
        "# Calculate the cosine similarity between the projected tweet embeddings and label embeddings\n",
        "cosine_similarities = np.dot(projected_tweet_embeddings, projected_label_embeddings.T) / (np.linalg.norm(projected_tweet_embeddings, axis=1)[:, np.newaxis] * np.linalg.norm(projected_label_embeddings, axis=1))\n",
        "# Generate label predictions\n",
        "label_predictions = np.argmax(cosine_similarities, axis=1)\n",
        "return label_predictions\n",
        "\n",
        "#Define main function\n",
        "def main():\n",
        "# Define tweet dataset and label names\n",
        "Advice = [\"['Stay at home']\",\"['wash hands']\",\"['wear mask']\",\"['social distancing']\"]\n",
        "China = [\"['Wuhan']\",\"['China Coronavirus Updates']\",\"['China news']\",\"['other tweets related to China']\"]\n",
        "Mask = [\"['Mask shortage']\",\"['wear mask']\",\"['mask types']\",\"['N50']\",\"['N95']\",\"['3M8210']\",\"['3M9001']\",\"['3M9322']\",\"['3M9501']\"]\n",
        "News = [\"['Coronavirus updates']\",\"['news']\",\"['rules']\"]\n",
        "Transportation = [\"['Flights']\",\"['traffic']\",\"['traveling']\"]\n",
        "USA = [\"['U.S. Coronavirus Updates']\",\"['COVID19']\",\"['U.S. news']\",\"['United States']\",\"['US']\",\"['USA']\"]\n",
        "Vaccine = [\"['Vaccine news']\",\"['vaccine progress']\",\"['vaccine injection']\"]\n",
        "tweets_df1 = pd.read_csv('tweets1.csv')\n",
        "tweets_df2 = pd.read_csv('tweets2.csv')\n",
        "tweets_df = pd.concat([tweets_df1, tweets_df2], ignore_index=True)\n",
        "hashtags = tweets_df['hashtags'].tolist()\n",
        "tweets = []\n",
        "for hashtag in hashtags:\n",
        "hashtag_list = eval(hashtag)\n",
        "tweet = ' '.join(hashtag_list)\n",
        "tweets.append(tweet)\n",
        "label_names = [\"Vaccine\",\"USA\",\"Transportation\",\"News\",\"Mask\",\"China\",\"Advice\"]\n",
        "# Load pre-trained S-BERT model and tokenizer\n",
        "model_name = 'sentence-transformers/bert-base-nli-mean-tokens'\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "# Obtain S-BERT embeddings for tweets and labels\n",
        "tweet_embeddings = obtain_sbert_embeddings(tweets, model, tokenizer)\n",
        "label_embeddings = obtain_sbert_embeddings(label_names, model, tokenizer)\n",
        "# Load ConceptNet knowledge graph embeddings\n",
        "kg_embedding_file = 'conceptnet_embedding.txt'\n",
        "kg_embedding = {}\n",
        "with open(kg_embedding_file, 'r', encoding='utf-8') as f:\n",
        "for line in f:\n",
        "values = line.split()\n",
        "word = values[0]\n",
        "embedding = np.asarray(values[1:], dtype='float32')\n",
        "kg_embedding[word] = embedding\n",
        "# Learn projection matrix\n",
        "projection_matrix = learn_projection_matrix(tweet_embeddings, kg_embedding)\n",
        "# Generate label predictions\n",
        "label_predictions = zero_shot_classification(tweet_embeddings, label_embeddings, projection_matrix)\n",
        "# Print label predictions\n",
        "for i in range(len(tweets)):\n",
        "print(\"Tweet: \", tweets[i])\n",
        "print(\"Predicted label: \", label_names[label_predictions[i]])\n",
        "\n",
        "#Call main function\n",
        "if name == 'main':\n",
        "main()"
      ],
      "metadata": {
        "id": "BjvuDCv9dgao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Python code for using pre-trained word embeddings and S-BERT for zero-shot classification\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "#Define function to obtain pre-trained word embeddings for sentences\n",
        "def obtain_word_embeddings(sentences, model, tokenizer):\n",
        "# Tokenize the sentences\n",
        "tokens = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "# Obtain the word embeddings for the tokens\n",
        "with torch.no_grad():\n",
        "embeddings = model(tokens['input_ids'], attention_mask=tokens['attention_mask'])[0]\n",
        "return embeddings\n",
        "\n",
        "#Define function for zero-shot classification using S-BERT\n",
        "def zero_shot_classification(tweet_embeddings, label_embeddings, label_names):\n",
        "# Calculate the cosine similarity between the tweet embeddings and label embeddings\n",
        "cosine_similarities = cosine_similarity(tweet_embeddings, label_embeddings)\n",
        "# Generate label predictions\n",
        "label_predictions = np.argmax(cosine_similarities, axis=1)\n",
        "label_predictions = [label_names[i] for i in label_predictions]\n",
        "return label_predictions\n",
        "\n",
        "#Define main function\n",
        "def main():\n",
        "# Define tweet dataset and label names\n",
        "label_names = [\"Vaccine\",\"USA\",\"Transportation\",\"News\",\"Mask\",\"China\",\"Advice\"]\n",
        "tweets_df1 = pd.read_csv('tweets1.csv')\n",
        "tweets_df2 = pd.read_csv('tweets2.csv')\n",
        "tweets_df = pd.concat([tweets_df1, tweets_df2], ignore_index=True)\n",
        "hashtags = tweets_df['hashtags'].tolist()\n",
        "tweets = []\n",
        "for hashtag in hashtags:\n",
        "hashtag_list = eval(hashtag)\n",
        "tweet = ' '.join(hashtag_list)\n",
        "tweets.append(tweet)\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "model_name = 'bert-base-uncased'\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "# Obtain pre-trained word embeddings for tweets and labels\n",
        "tweet_embeddings = obtain_word_embeddings(tweets, model, tokenizer)\n",
        "label_embeddings = obtain_word_embeddings(label_names, model, tokenizer)\n",
        "# Use S-BERT for zero-shot classification\n",
        "label_predictions = zero_shot_classification(tweet_embeddings, label_embeddings, label_names)\n",
        "# Print label predictions\n",
        "for i in range(len(tweets)):\n",
        "print(\"Tweet: \", tweets[i])\n",
        "print(\"Predicted label: \", label_predictions[i])\n",
        "\n",
        "#Call main function\n",
        "if name == 'main':\n",
        "main()"
      ],
      "metadata": {
        "id": "yFnmCUZwe2FE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Python code for using ConceptNet knowledge graph embeddings for zero-shot classification\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "#Define function to obtain pre-trained word embeddings for sentences\n",
        "def obtain_word_embeddings(sentences, model, tokenizer):\n",
        "# Tokenize the sentences\n",
        "tokens = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "# Obtain the word embeddings for the tokens\n",
        "with torch.no_grad():\n",
        "embeddings = model(tokens['input_ids'], attention_mask=tokens['attention_mask'])[0]\n",
        "return embeddings\n",
        "\n",
        "#Define function to learn projection matrix using retrofitting\n",
        "def learn_projection_matrix(tweet_embeddings, kg_embedding):\n",
        "# Initialize word embeddings with ConceptNet embeddings\n",
        "word_embeddings = kg_embedding.copy()\n",
        "# Define hyperparameters\n",
        "alpha = 0.5\n",
        "beta = 0.5\n",
        "num_iterations = 5\n",
        "# Retrofit word embeddings with tweet embeddings\n",
        "for i in range(num_iterations):\n",
        "for word in word_embeddings:\n",
        "neighbors = word_embeddings[word]\n",
        "if word in tweet_embeddings:\n",
        "neighbors = np.concatenate((neighbors, alphatweet_embeddings[word]), axis=0) new_embedding = np.mean(neighbors, axis=0) word_embeddings[word] = betakg_embedding[word] + (1-beta)*new_embedding\n",
        "# Learn projection matrix\n",
        "projection_matrix = np.linalg.pinv(word_embeddings)\n",
        "return projection_matrix\n",
        "\n",
        "#Define function for zero-shot classification using retrofitting and S-BERT\n",
        "def zero_shot_classification(tweet_embeddings, label_embeddings, projection_matrix):\n",
        "# Project tweet embeddings onto ConceptNet semantic space\n",
        "tweet_embeddings = np.matmul(tweet_embeddings, projection_matrix)\n",
        "# Calculate the cosine similarity between the tweet embeddings and label embeddings\n",
        "cosine_similarities = cosine_similarity(tweet_embeddings, label_embeddings)\n",
        "# Generate label predictions\n",
        "label_predictions = np.argmax(cosine_similarities, axis=1)\n",
        "return label_predictions\n",
        "\n",
        "#Define main function\n",
        "def main():\n",
        "# Define tweet dataset and label names\n",
        "label_names = [\"Vaccine\",\"USA\",\"Transportation\",\"News\",\"Mask\",\"China\",\"Advice\"]\n",
        "tweets_df1 = pd.read_csv('tweets1.csv')\n",
        "tweets_df2 = pd.read_csv('tweets2.csv')\n",
        "tweets_df = pd.concat([tweets_df1, tweets_df2], ignore_index=True)\n",
        "hashtags = tweets_df['hashtags'].tolist()\n",
        "tweets = []\n",
        "for hashtag in hashtags:\n",
        "hashtag_list = eval(hashtag)\n",
        "tweet = ' '.join(hashtag_list)\n",
        "tweets.append(tweet)\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "model_name = 'bert-base-uncased'\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "# Obtain pre-trained word embeddings for tweets\n",
        "tweet_embeddings = obtain_word_embeddings(tweets, model, tokenizer)\n",
        "# Load ConceptNet knowledge graph embeddings\n",
        "kg_embedding_file = 'conceptnet_embedding.txt'\n",
        "kg_embedding = {}\n",
        "with open(kg_embedding_file, 'r', encoding='utf-8') as f:\n",
        "for line in f:\n",
        "values = line.split()\n",
        "word = values[0]\n",
        "embedding = np.asarray(values[1:], dtype='float32')\n",
        "kg_embedding[word] = embedding\n",
        "# Learn projection matrix using retrofitting\n",
        "projection_matrix = learn_projection_matrix(tweet_embeddings, kg_embedding)\n",
        "# Obtain ConceptNet embeddings for labels\n",
        "label_embeddings = np.matmul(kg_embedding[label_names], projection_matrix)\n",
        "# Use S-BERT for zero-shot classification\n",
        "label_predictions = zero_shot_classification(tweet_embeddings, label_embeddings, projection_matrix)\n",
        "# Print label predictions\n",
        "for i in range(len(tweets)):\n",
        "print(\"Tweet: \", tweets[i])\n",
        "print(\"Predicted label: \", label_names[label_predictions[i]])\n",
        "\n",
        "#Call main function\n",
        "if name == 'main':\n",
        "main()"
      ],
      "metadata": {
        "id": "eKNZQWLofJzK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C6i9aeX2czsr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "#Define function to calculate knowledge graph embedding\n",
        "def calculate_kg_embedding(omega, glove_embedding):\n",
        "# Calculate the average of the GloVe embeddings for each word in the knowledge graph\n",
        "kg_embedding = {}\n",
        "for word in omega:\n",
        "if word in glove_embedding:\n",
        "kg_embedding[word] = np.mean(glove_embedding[word], axis=0)\n",
        "return kg_embedding\n",
        "\n",
        "#Define function to obtain S-BERT embeddings for words\n",
        "def obtain_sbert_embeddings(words, model, tokenizer):\n",
        "# Tokenize the words\n",
        "tokens = tokenizer(words, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "# Obtain the S-BERT embeddings for the tokens\n",
        "with torch.no_grad():\n",
        "embeddings = model(tokens['input_ids'], attention_mask=tokens['attention_mask'])[0]\n",
        "return embeddings\n",
        "\n",
        "#Define function to learn a least-squares linear projection matrix\n",
        "def learn_projection_matrix(sbert_embeddings, kg_embeddings):\n",
        "# Convert the embeddings to numpy arrays\n",
        "sbert_embeddings = sbert_embeddings.detach().numpy()\n",
        "kg_embeddings = np.array([kg_embeddings[word] for word in sbert_embeddings])\n",
        "# Learn the projection matrix using linear regression\n",
        "reg = LinearRegression().fit(sbert_embeddings, kg_embeddings)\n",
        "projection_matrix = reg.coef_\n",
        "return projection_matrix\n",
        "\n",
        "#Define function for zero-shot text classification\n",
        "def zero_shot_classification(tweet_embeddings, label_embeddings, projection_matrix):\n",
        "# Project the tweet embeddings and label embeddings into the knowledge graph embedding space\n",
        "projected_tweet_embeddings = np.dot(tweet_embeddings, projection_matrix)\n",
        "projected_label_embeddings = np.dot(label_embeddings, projection_matrix)\n",
        "# Calculate the cosine similarity between the projected tweet embeddings and label embeddings\n",
        "cosine_similarities = np.dot(projected_tweet_embeddings, projected_label_embeddings.T) / (np.linalg.norm(projected_tweet_embeddings, axis=1)[:, np.newaxis] * np.linalg.norm(projected_label_embeddings, axis=1))\n",
        "# Generate label predictions\n",
        "label_predictions = np.argmax(cosine_similarities, axis=1)\n",
        "return label_predictions\n",
        "\n",
        "#Define main function\n",
        "def main():\n",
        "# Load pre-trained S-BERT model and tokenizer\n",
        "model_name = 'sentence-transformers/bert-base-nli-mean-tokens'\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "# Load GloVe word embeddings\n",
        "glove_embedding_file = 'glove.6B.300d.txt'\n",
        "glove_embedding = {}\n",
        "with open(glove_embedding_file, 'r', encoding='utf-8') as f:\n",
        "for line in f:\n",
        "values = line.split()\n",
        "word = values[0]\n",
        "embedding = np.asarray(values[1:], dtype='float32')\n",
        "glove_embedding[word] = embedding\n",
        "# Define knowledge graph\n",
        "omega = ['knowledge', 'graph', 'embedding']\n",
        "# Calculate knowledge graph embedding\n",
        "kg_embedding = calculate_kg_embedding(omega, glove_embedding)\n",
        "# Define vocabulary words\n",
        "k = 100\n",
        "vocabulary_words = list(model.tokenizer.get_vocab().keys())[:k]\n",
        "# Obtain S-BERT embeddings for vocabulary words\n",
        "sbert_embeddings = obtain_sbert_embeddings(vocabulary_words, model, tokenizer)\n",
        "# Learn projection matrix\n",
        "projection_matrix = learn_projection_matrix(sbert_embeddings, kg_embedding)\n",
        "# Define tweet dataset and label names\n",
        "tweet_dataset = ['This is a tweet about knowledge graphs', 'I love learning about embeddings', 'S-BERT is a great tool for natural language processing']\n",
        "label_names = ['knowledge', 'graph', 'embedding', 'natural', 'language', 'processing']\n",
        "# Obtain S-BERT embeddings for tweets and labels\n",
        "tweet_embeddings = obtain_sbert_embeddings(tweet_dataset, model, tokenizer)\n",
        "label_embeddings = obtain_sbert_embeddings(label_names, model, tokenizer)\n",
        "# Generate label predictions\n",
        "label_predictions = zero_shot_classification(tweet_embeddings, label_embeddings, projection_matrix)\n",
        "print(label_predictions)\n",
        "\n",
        "#Call main function\n",
        "if name == 'main':\n",
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Python code for implementing baseline models and S-BERT-KG for zero-shot\n",
        "Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "#Define function to obtain pre-trained word embeddings for sentences\n",
        "def obtain_word_embeddings(sentences, model, tokenizer):\n",
        "# Tokenize the sentences\n",
        "tokens = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "# Obtain the word embeddings for the tokens\n",
        "with torch.no_grad():\n",
        "embeddings = model(tokens['input_ids'], attention_mask=tokens['attention_mask'])[0]\n",
        "return embeddings\n",
        "\n",
        "#Define function for GloVe-AVG baseline model\n",
        "def glove_avg_classification(tweet_embeddings, label_embeddings):\n",
        "# Calculate the cosine similarity between the tweet embeddings and label embeddings\n",
        "cosine_similarities = cosine_similarity(tweet_embeddings.mean(axis=1), label_embeddings)\n",
        "# Generate label predictions\n",
        "label_predictions = np.argmax(cosine_similarities, axis=1)\n",
        "return label_predictions\n",
        "\n",
        "#Define function for BERT-CLS baseline model\n",
        "def bert_cls_classification(tweet_embeddings, label_embeddings):\n",
        "# Calculate the cosine similarity between the tweet embeddings and label embeddings\n",
        "cosine_similarities = cosine_similarity(tweet_embeddings[:, 0, :], label_embeddings)\n",
        "# Generate label predictions\n",
        "label_predictions = np.argmax(cosine_similarities, axis=1)\n",
        "return label_predictions\n",
        "\n",
        "#Define function for BERT-AVG baseline model\n",
        "def bert_avg_classification(tweet_embeddings, label_embeddings):\n",
        "# Calculate the cosine similarity between the tweet embeddings and label embeddings\n",
        "cosine_similarities = cosine_similarity(tweet_embeddings.mean(axis=1), label_embeddings)\n",
        "# Generate label predictions\n",
        "label_predictions = np.argmax(cosine_similarities, axis=1)\n",
        "return label_predictions\n",
        "\n",
        "#Define function for S-BERT baseline model\n",
        "def sbert_classification(tweet_embeddings, label_embeddings):\n",
        "# Calculate the cosine similarity between the tweet embeddings and label embeddings\n",
        "cosine_similarities = cosine_similarity(tweet_embeddings, label_embeddings)\n",
        "# Generate label predictions\n",
        "label_predictions = np.argmax(cosine_similarities, axis=1)\n",
        "return label_predictions\n",
        "\n",
        "#Define function for S-BERT-GloVe baseline model\n",
        "def sbert_glove_classification(tweet_embeddings, label_embeddings, projection_matrix):\n",
        "# Project S-BERT embeddings onto GloVe semantic space\n",
        "tweet_embeddings = np.matmul(tweet_embeddings, projection_matrix)\n",
        "label_embeddings = np.matmul(label_embeddings, projection_matrix)\n",
        "# Calculate the cosine similarity between the tweet embeddings and label embeddings\n",
        "cosine_similarities = cosine_similarity(tweet_embeddings, label_embeddings)\n",
        "# Generate label predictions\n",
        "label_predictions = np.argmax(cosine_similarities, axis=1)\n",
        "return label_predictions\n",
        "\n",
        "#Define function for BART-NLI baseline model\n",
        "def bart_nli_classification(tweets, label_names, model, tokenizer):\n",
        "# Construct hypothesis for each label\n",
        "hypotheses = [f\"The text is about {label}.\" for label in label_names]\n",
        "# Tokenize the tweets and hypotheses\n",
        "tokens = tokenizer(tweets, hypotheses, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "# Obtain the logits for each label\n",
        "with torch.no_grad():\n",
        "outputs = model(input_ids=tokens['input_ids'], attention_mask=tokens['attention_mask'], labels=None)\n",
        "logits = outputs.logits\n",
        "# Generate label predictions\n",
        "label_predictions = np.argmax(logits.detach().numpy(), axis=1)\n",
        "return label_predictions\n",
        "\n",
        "#Define function for S-BERT-KG model\n",
        "def sbert_kg_classification(tweet_embeddings, label_embeddings, projection_matrix):\n",
        "# Project S-BERT embeddings onto ConceptNet semantic space\n",
        "tweet_embeddings = np.matmul(tweet_embeddings, projection_matrix)\n",
        "label_embeddings = np.matmul(label_embeddings, projection_matrix)\n",
        "# Calculate the cosine similarity between the tweet embeddings and label embeddings\n",
        "cosine_similarities = cosine_similarity(tweet_embeddings, label_embeddings)\n",
        "# Generate label predictions\n",
        "label_predictions = np.argmax(cosine_similarities, axis=1)\n",
        "return label_predictions\n",
        "\n",
        "#Define main function\n",
        "def main():\n",
        "# Define tweet dataset and label names\n",
        "label_names = [\"Vaccine\",\"USA\",\"Transportation\",\"News\",\"Mask\",\"China\",\"Advice\"]\n",
        "tweets_df1 = pd.read_csv('tweets1.csv')\n",
        "tweets_df2 = pd.read_csv('tweets2.csv')\n",
        "tweets_df = pd.concat([tweets_df1, tweets_df2], ignore_index=True)\n",
        "hashtags = tweets_df['hashtags'].tolist()\n",
        "tweets = []\n",
        "for hashtag in hashtags:\n",
        "hashtag_list = eval(hashtag)\n",
        "tweet = ' '.join(hashtag_list)\n",
        "tweets.append(tweet)\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "model_name = 'bert-base-uncased'\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name\n",
        "# Load pre-trained GloVe embeddings\n",
        "glove_embeddings = pd.read_csv('glove.6B.300d.txt', sep=\" \", quoting=3, header=None, index_col=0)\n",
        "glove_embeddings = {key: val.values for key, val in glove_embeddings.T.items()}\n",
        "# Obtain GloVe embeddings for label names\n",
        "label_embeddings = np.array([glove_embeddings[label] for label in label_names])\n",
        "# Obtain word embeddings for tweets\n",
        "tweet_embeddings = obtain_word_embeddings(tweets, model, tokenizer)\n",
        "# Obtain projection matrix for S-BERT-GloVe and S-BERT-KG models\n",
        "projection_matrix = np.load('projection_matrix.npy')\n",
        "# Generate label predictions for each baseline model and S-BERT-KG model\n",
        "glove_avg_predictions = glove_avg_classification(tweet_embeddings, label_embeddings)\n",
        "bert_cls_predictions = bert_cls_classification(tweet_embeddings, label_embeddings)\n",
        "bert_avg_predictions = bert_avg_classification(tweet_embeddings, label_embeddings)\n",
        "sbert_predictions = sbert_classification(tweet_embeddings, label_embeddings)\n",
        "sbert_glove_predictions = sbert_glove_classification(tweet_embeddings, label_embeddings, projection_matrix)\n",
        "bart_nli_predictions = bart_nli_classification(tweets, label_names, model, tokenizer)\n",
        "sbert_kg_predictions = sbert_kg_classification(tweet_embeddings, label_embeddings, projection_matrix)\n",
        "# Print accuracy scores for each model\n",
        "print(\"Accuracy scores for baseline models:\")\n",
        "print(\"GloVe-AVG:\", np.mean(glove_avg_predictions == tweets_df['label']))\n",
        "print(\"BERT-CLS:\", np.mean(bert_cls_predictions == tweets_df['label']))\n",
        "print(\"BERT-AVG:\", np.mean(bert_avg_predictions == tweets_df['label']))\n",
        "print(\"S-BERT:\", np.mean(sbert_predictions == tweets_df['label']))\n",
        "print(\"S-BERT-GloVe:\", np.mean(sbert_glove_predictions == tweets_df['label']))\n",
        "print(\"BART-NLI:\", np.mean(bart_nli_predictions == tweets_df['label']))\n",
        "print(\"Accuracy score for S-BERT-KG model:\", np.mean(sbert_kg_predictions == tweets_df['label']))\n"
      ],
      "metadata": {
        "id": "nyJ0WMNriUEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Python code for comparing performance of different models for zero-shot multiclass and multilabel classification\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, hamming_loss\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from scipy.spatial.distance import cosine\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "#Define function to obtain pre-trained word embeddings for sentences\n",
        "def obtain_word_embeddings(sentences, model, tokenizer):\n",
        "# Tokenize the sentences\n",
        "tokens = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "# Obtain the word embeddings for the tokens\n",
        "with torch.no_grad():\n",
        "embeddings = model(tokens['input_ids'], attention_mask=tokens['attention_mask'])[0]\n",
        "return embeddings\n",
        "\n",
        "#Define function for GloVe-AVG baseline model\n",
        "def glove_avg_classification(tweet_embeddings, label_embeddings):\n",
        "# Calculate the cosine similarity between the tweet embeddings and label embeddings\n",
        "cosine_similarities = cosine_similarity(tweet_embeddings.mean(axis=1), label_embeddings)\n",
        "# Generate label predictions\n",
        "label_predictions = np.argmax(cosine_similarities, axis=1)\n",
        "return label_predictions\n",
        "\n",
        "#Define function for BERT-CLS baseline model\n",
        "def bert_cls_classification(tweet_embeddings, label_embeddings):\n",
        "# Calculate the cosine similarity between the tweet embeddings and label embeddings\n",
        "cosine_similarities = cosine_similarity(tweet_embeddings[:, 0, :], label_embeddings)\n",
        "# Generate label predictions\n",
        "label_predictions = np.argmax(cosine_similarities, axis=1)\n",
        "return label_predictions\n",
        "\n",
        "#Define function for BERT-AVG baseline model\n",
        "def bert_avg_classification(tweet_embeddings, label_embeddings):\n",
        "# Calculate the cosine similarity between the tweet embeddings and label embeddings\n",
        "cosine_similarities = cosine_similarity(tweet_embeddings.mean(axis=1), label_embeddings)\n",
        "# Generate label predictions\n",
        "label_predictions = np.argmax(cosine_similarities, axis=1)\n",
        "return label_predictions\n",
        "\n",
        "#Define function for S-BERT baseline model\n",
        "def sbert_classification(tweet_embeddings, label_embeddings):\n",
        "# Calculate the cosine similarity between the tweet embeddings and label embeddings\n",
        "cosine_similarities = cosine_similarity(tweet_embeddings, label_embeddings)\n",
        "# Generate label predictions\n",
        "label_predictions = np.argmax(cosine_similarities, axis=1)\n",
        "return label_predictions\n",
        "\n",
        "#Define function for S-BERT-GloVe baseline model\n",
        "def sbert_glove_classification(tweet_embeddings, label_embeddings, projection_matrix):\n",
        "# Project S-BERT embeddings onto GloVe semantic space\n",
        "tweet_embeddings = np.matmul(tweet_embeddings, projection_matrix)\n",
        "label_embeddings = np.matmul(label_embeddings, projection_matrix)\n",
        "# Calculate the cosine similarity between the tweet embeddings and label embeddings\n",
        "cosine_similarities = cosine_similarity(tweet_embeddings, label_embeddings)\n",
        "# Generate label predictions\n",
        "label_predictions = np.argmax(cosine_similarities, axis=1)\n",
        "return label_predictions\n",
        "\n",
        "#Define function for BART-NLI baseline model\n",
        "def bart_nli_classification(tweets, label_names, model, tokenizer):\n",
        "# Construct hypothesis for each label\n",
        "hypotheses = [f\"The text is about {label}.\" for label in label_names]\n",
        "# Tokenize the tweets and hypotheses\n",
        "tokens = tokenizer(tweets, hypotheses, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "# Obtain the logits for each label\n",
        "with torch.no_grad():\n",
        "outputs = model(input_ids=tokens['input_ids'], attention_mask=tokens['attention_mask'], labels=None)\n",
        "logits = outputs.logits\n",
        "# Generate label predictions\n",
        "label_predictions = np.argmax(logits.detach().numpy(), axis=1)\n",
        "return label_predictions\n",
        "\n",
        "#Define function for S-BERT-KG model\n",
        "def sbert_kg_classification(tweet_embeddings, label_embeddings, projection_matrix):\n",
        "# Project S-BERT embeddings onto ConceptNet semantic space\n",
        "tweet_embeddings = np.matmul(tweet_embeddings, projection_matrix)\n",
        "label_embeddings = np.matmul(label_embeddings, projection_matrix)\n",
        "# Calculate the cosine similarity between the tweet embeddings and label embeddings\n",
        "cosine_similarities = cosine_similarity(tweet_embeddings, label_embeddings)\n",
        "# Generate label predictions\n",
        "label_predictions = np.argmax(cosine_similarities, axis=1)\n",
        "return label_predictions\n",
        "\n",
        "#Define function for evaluating model performance\n",
        "def evaluate_model_performance(model, tweets, labels, label_names, mlb):\n",
        "# Obtain word embeddings for tweets\n",
        "tweet_embeddings = model.encode(tweets)\n",
        "# Obtain label embeddings\n",
        "label_embeddings = np.array([model.encode([label])[0] for label in label_names])\n",
        "# Generate label predictions\n",
        "label_predictions = model(tweet_embeddings, label_embeddings)\n",
        "# Convert label predictions to binary format for multilabel classification\n",
        "label_predictions_binary = mlb.transform(label_predictions)\n",
        "# Calculate evaluation metrics\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(labels, label_predictions_binary, average='weighted')\n",
        "accuracy= accuracy_score(labels, label_predictions)\n",
        "hamming_loss_value = hamming_loss(labels, label_predictions_binary)\n",
        "running_time = time.time() - start_time\n",
        "# Return evaluation metrics\n",
        "return precision, recall, f1, accuracy, hamming_loss_value, running_time\n",
        "Load pre-trained models and tokenizers\n",
        "glove_model = pd.read_csv('glove.6B.300d.txt', sep=\" \", index_col=0, header=None, quoting=3)\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_cls_model = AutoModel.from_pretrained('bert-base-uncased')\n",
        "bert_avg_model = AutoModel.from_pretrained('bert-base-uncased')\n",
        "sbert_model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "sbert_glove_model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "sbert_kg_model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "bart_nli_model = AutoModel.from_pretrained('facebook/bart-large-mnli')\n",
        "\n",
        "#Load data\n",
        "tweets = pd.read_csv('tweets.csv')\n",
        "labels = pd.read_csv('labels.csv')\n",
        "\n",
        "#Define label names\n",
        "label_names = ['politics', 'sports', 'entertainment', 'technology', 'business', 'health', 'education']\n",
        "\n",
        "#Convert labels to binary format for multilabel classification\n",
        "mlb = MultiLabelBinarizer()\n",
        "labels_binary = mlb.fit_transform(labels)\n",
        "\n",
        "#Obtain word embeddings for GloVe model\n",
        "glove_embeddings = glove_model.loc[label_names].values\n",
        "\n",
        "#Obtain word embeddings for BERT-CLS and BERT-AVG models\n",
        "bert_cls_embeddings = obtain_word_embeddings(label_names, bert_cls_model, tokenizer)\n",
        "bert_avg_embeddings = obtain_word_embeddings(label_names, bert_avg_model, tokenizer)\n",
        "\n",
        "#Obtain word embeddings for S-BERT, S-BERT-GloVe, and S-BERT-KG models\n",
        "sbert_embeddings = sbert_model.encode(label_names)\n",
        "sbert_glove_projection_matrix = np.random.rand(768, 20000)\n",
        "sbert_glove_embeddings = sbert_glove_model.encode(label_names)\n",
        "sbert_kg_projection_matrix = np.random.rand(768, 20000)\n",
        "sbert_kg_embeddings = sbert_kg_model.encode(label_names)\n",
        "\n",
        "#Obtain word embeddings for BART-NLI model\n",
        "bart_nli_embeddings = obtain_word_embeddings(label_names, bart_nli_model, tokenizer)\n",
        "\n",
        "#Evaluate performance of GloVe model\n",
        "glove_precision, glove_recall, glove_f1, glove_accuracy, glove_hamming_loss, glove_running_time = evaluate_model_performance(glove_avg_classification, tweets, labels_binary, label_names, mlb)\n",
        "\n",
        "#Evaluate performance of BERT-CLS model\n",
        "bert_cls_precision, bert_cls_recall, bert_cls_f1, bert_cls_accuracy, bert_cls_hamming_loss, bert_cls_running_time = evaluate_model_performance(bert_cls_classification, tweets, labels_binary, bert_cls_embeddings, mlb)\n",
        "\n",
        "#Evaluate performance of BERT-AVG model\n",
        "bert_avg_precision, bert_avg_recall, bert_avg_f1, bert_avg_accuracy, bert_avg_hamming_loss, bert_avg_running_time = evaluate_model_performance(bert_avg_classification, tweets, labels_binary, bert_avg_embeddings, mlb)\n",
        "\n",
        "#Evaluate performance of S-BERT model\n",
        "sbert_precision, sbert_recall, sbert_f1, sbert_accuracy, sbert_hamming_loss, sbert_running_time = evaluate_model_performance(sbert_classification, tweets, labels_binary, sbert_embeddings, mlb)\n",
        "\n",
        "#Evaluate performance of S-BERT-GloVe model\n",
        "sbert_glove_precision, sbert_glove_recall, sbert_glove_f1, sbert_glove_accuracy, sbert_glove_hamming_loss, sbert_glove_running_time = evaluate_model_performance(sbert_glove_classification, tweets, labels_binary, sbert_glove_embeddings, mlb)\n",
        "\n",
        "#Evaluate performance of BART-NLI model\n",
        "bart_nli_precision, bart_nli_recall, bart_nli_f1, bart_nli_accuracy, bart_nli_hamming_loss, bart_nli_running_time = evaluate_model_performance(bart_nli_classification, tweets, labels_binary, label_names, mlb)\n",
        "\n",
        "#Evaluate performance of S-BERT-KG model\n",
        "sbert_kg_precision, sbert_kg_recall, sbert_kg_f1, sbert_kg_accuracy, sbert_kg_hamming_loss, sbert_kg_running_time = evaluate_model_performance(sbert_kg_classification, tweets, labels_binary, sbert_kg_embeddings, mlb)\n",
        "\n",
        "#Create dataframe to store evaluation metrics\n",
        "evaluation_metrics = pd.DataFrame({'Model': ['GloVe', 'BERT-CLS', 'BERT-AVG', 'S-BERT', 'S-BERT-GloVe', 'BART-NLI', 'S-BERT-KG'],\n",
        "'Precision': [glove_precision, bert_cls_precision, bert_avg_precision, sbert_precision, sbert_glove_precision, bart_nli_precision, sbert_kg_precision],\n",
        "'Recall': [glove_recall, bert_cls_recall, bert_avg_recall, sbert_recall, sbert_glove_recall, bart_nli_recall, sbert_kg_recall],\n",
        "'F1 Score': [glove_f1, bert_cls_f1, bert_avg_f1, sbert_f1, sbert_glove_f1, bart_nli_f1, sbert_kg_f1],\n",
        "'Accuracy': [glove_accuracy, bert_cls_accuracy, bert_avg_accuracy, sbert_accuracy, sbert_glove_accuracy, bart_nli_accuracy, sbert_kg_accuracy],\n",
        "'Hamming Loss': [glove_hamming_loss, bert_cls_hamming_loss, bert_avg_hamming_loss, sbert_hamming_loss, sbert_glove_hamming_loss, bart_nli_hamming_loss, sbert_kg_hamming_loss],\n",
        "'Running Time (s)': [glove_running_time, bert_cls_running_time, bert_avg_running_time, sbert_running_time, sbert_glove_running_time, bart_nli_running_time, sbert_kg_running_time]})\n",
        "\n",
        "#Print evaluation metrics for multiclass classification\n",
        "print(\"Evaluation Metrics for Zero-Shot Multiclass Classification:\")\n",
        "print(evaluation_metrics)\n",
        "\n",
        "#Evaluate performance of models for multilabel classification\n",
        "glove_precision, glove_recall, glove_f1, glove_accuracy, glove_hamming_loss, glove_running_time = evaluate_model_performance(glove_avg_classification, tweets, labels_binary, label_names, mlb)\n",
        "bert_cls_precision, bert_cls_recall, bert_cls_f1, bert_cls_accuracy, bert_cls_hamming_loss, bert_cls_running_time = evaluate_model_performance(bert_cls_classification, tweets, labels_binary, bert_cls_embeddings, mlb)\n",
        "bert_avg_precision, bert_avg_recall, bert_avg_f1, bert_avg_accuracy, bert_avg_hamming_loss, bert_avg_running_time = evaluate_model_performance(bert_avg_classification, tweets, labels_binary, bert_avg_embeddings, mlb)\n",
        "sbert_precision, sbert_recall, sbert_f1, sbert_accuracy, sbert_hamming_loss, sbert_running_time = evaluate_model_performance(sbert_classification, tweets, labels_binary, sbert_embeddings, mlb)\n",
        "sbert_glove_precision, sbert_glove_recall, sbert_glove_f1, sbert_glove_accuracy, sbert_glove_hamming_loss, sbert_glove_running_time = evaluate_model_performance(sbert_glove_classification, tweets, labels_binary, sbert_glove_embeddings, mlb)\n",
        "bart_nli_precision, bart_nli_recall, bart_nli_f1, bart_nli_accuracy, bart_nli_hamming_loss, bart_nli_running_time = evaluate_model_performance(bart_nli_classification, tweets, labels_binary, label_names, mlb)\n",
        "sbert_kg_precision, sbert_kg_recall, sbert_kg_f1, sbert_kg_accuracy, sbert_kg_hamming_loss, sbert_kg_running_time = evaluate_model_performance(sbert_kg_classification, tweets, labels_binary, sbert_kg_embeddings, mlb)\n",
        "\n",
        "#Create dataframe to store evaluation metrics\n",
        "evaluation_metrics = pd.DataFrame({'Model': ['GloVe', 'BERT-CLS', 'BERT-AVG', 'S-BERT', 'S-BERT-GloVe', 'BART-NLI', 'S-BERT-KG'],\n",
        "'Precision': [glove_precision, bert_cls_precision, bert_avg_precision, sbert_precision, sbert_glove_precision, bart_nli_precision, sbert_kg_precision],\n",
        "'Recall': [glove_recall, bert_cls_recall, bert_avg_recall, sbert_recall, sbert_glove_recall, bart_nli_recall, sbert_kg_recall],\n",
        "'F1 Score': [glove_f1, bert_cls_f1, bert_avg_f1, sbert_f1, sbert_glove_f1, bart_nli_f1, sbert_kg_f1],\n",
        "'Accuracy': [glove_accuracy, bert_cls_accuracy, bert_avg_accuracy, sbert_accuracy, sbert_glove_accuracy, bart_nli_accuracy, sbert_kg_accuracy],\n",
        "'Hamming Loss': [glove_hamming_loss, bert_cls_hamming_loss, bert_avg_hamming_loss, sbert_hamming_loss, sbert_glove_hamming_loss, bart_nli_hamming_loss, sbert_kg_hamming_loss],\n",
        "'Running Time (s)': [glove_running_time, bert_cls_running_time, bert_avg_running_time, sbert_running_time, sbert_glove_running_time, bart_nli_running_time, sbert_kg_running_time]})\n",
        "\n",
        "#Print evaluation metrics for multilabel classification\n",
        "print(\"Evaluation Metrics for Zero-Shot Multilabel Classification:\")\n",
        "print(evaluation_metrics)"
      ],
      "metadata": {
        "id": "71A_vUaEiqkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, hamming_loss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from scipy.spatial.distance import cosine\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Load data\n",
        "tweets = pd.read_csv('tweets.csv')\n",
        "labels = pd.read_csv('labels.csv')\n",
        "\n",
        "#Define label names\n",
        "label_names = ['politics', 'sports', 'entertainment', 'technology', 'business', 'health', 'education']\n",
        "\n",
        "#Convert labels to binary format for multilabel classification\n",
        "mlb = MultiLabelBinarizer()\n",
        "labels_binary = mlb.fit_transform(labels)\n",
        "\n",
        "#Obtain word embeddings for GloVe model\n",
        "glove_embeddings = glove_model.loc[label_names].values\n",
        "\n",
        "#Obtain word embeddings for BERT-CLS and BERT-AVG models\n",
        "bert_cls_model = AutoModel.from_pretrained('bert-base-uncased')\n",
        "bert_cls_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_cls_embeddings = obtain_word_embeddings(label_names, bert_cls_model, bert_cls_tokenizer)\n",
        "bert_avg_model = AutoModel.from_pretrained('bert-base-uncased')\n",
        "bert_avg_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_avg_embeddings = obtain_word_embeddings(label_names, bert_avg_model, bert_avg_tokenizer)\n",
        "\n",
        "#Obtain word embeddings for S-BERT, S-BERT-GloVe, and S-BERT-KG models\n",
        "sbert_model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "sbert_embeddings = sbert_model.encode(label_names)\n",
        "sbert_glove_projection_matrix = np.random.rand(768, 20000)\n",
        "sbert_glove_embeddings = sbert_glove_model.encode(label_names)\n",
        "sbert_kg_projection_matrix = np.random.rand(768, 20000)\n",
        "sbert_kg_embeddings = sbert_kg_model.encode(label_names)\n",
        "\n",
        "#Obtain word embeddings for BART-NLI model\n",
        "bart_nli_model = AutoModel.from_pretrained('facebook/bart-large-nli-stsb-mean-tokens')\n",
        "bart_nli_tokenizer = AutoTokenizer.from_pretrained('facebook/bart-large-nli-stsb-mean-tokens')\n",
        "bart_nli_embeddings = obtain_word_embeddings(label_names, bart_nli_model, bart_nli_tokenizer)\n",
        "\n",
        "#Evaluate performance of models for multiclass classification\n",
        "glove_precision, glove_recall, glove_f1, glove_accuracy, glove_hamming_loss, glove_running_time = evaluate_model_performance(glove_avg_classification, tweets, labels_binary, label_names, mlb)\n",
        "bert_cls_precision, bert_cls_recall, bert_cls_f1, bert_cls_accuracy, bert_cls_hamming_loss, bert_cls_running_time = evaluate_model_performance(bert_cls_classification, tweets, labels_binary, bert_cls_embeddings, mlb)\n",
        "bert_avg_precision, bert_avg_recall, bert_avg_f1, bert_avg_accuracy, bert_avg_hamming_loss, bert_avg_running_time = evaluate_model_performance(bert_avg_classification, tweets, labels_binary, bert_avg_embeddings, mlb)\n",
        "sbert_precision, sbert_recall, sbert_f1, sbert_accuracy, sbert_hamming_loss, sbert_running_time = evaluate_model_performance(sbert_classification, tweets, labels_binary, sbert_embeddings, mlb)\n",
        "sbert_glove_precision, sbert_glove_recall, sbert_glove_f1, sbert_glove_accuracy, sbert_glove_hamming_loss, sbert_glove_running_time = evaluate_model_performance(sbert_glove_classification, tweets, labels_binary, sbert_glove_embeddings, mlb)\n",
        "bart_nli_precision, bart_nli_recall, bart_nli_f1, bart_nli_accuracy, bart_nli_hamming_loss, bart_nli_running_time = evaluate_model_performance(bart_nli_classification, tweets, labels_binary, label_names, mlb)\n",
        "sbert_kg_precision, sbert_kg_recall, sbert_kg_f1, sbert_kg_accuracy, sbert_kg_hamming_loss, sbert_kg_running_time = evaluate_model_performance(sbert_kg_classification, tweets, labels_binary, sbert_kg_embeddings, mlb)\n",
        "\n",
        "#Create dataframe to store evaluation metrics\n",
        "evaluation_metrics = pd.DataFrame({'Model': ['GloVe', 'BERT-CLS', 'BERT-AVG', 'S-BERT', 'S-BERT-GloVe', 'BART-NLI', 'S-BERT-KG'],\n",
        "'Precision': [glove_precision, bert_cls_precision, bert_avg_precision, sbert_precision, sbert_glove_precision, bart_nli_precision, sbert_kg_precision],\n",
        "'Recall': [glove_recall, bert_cls_recall, bert_avg_recall, sbert_recall, sbert_glove_recall, bart_nli_recall, sbert_kg_recall],\n",
        "'F1 Score':[glove_f1, bert_cls_f1, bert_avg_f1,ert_f1, sbert_glove_f1, bart_nli_f1, sbert_kg_f1],\n",
        "'Accuracy': [glove_accuracy, bert_cls_accuracy, bert_avg_accuracy, sbert_accuracy, sbert_glove_accuracy, bart_nli_accuracy, sbert_kg_accuracy],\n",
        "'Hamming Loss': [glove_hamming_loss, bert_cls_hamming_loss, bert_avg_hamming_loss, sbert_hamming_loss, sbert_glove_hamming_loss, bart_nli_hamming_loss, sbert_kg_hamming_loss],\n",
        "'Running Time (s)': [glove_running_time, bert_cls_running_time, bert_avg_running_time, sbert_running_time, sbert_glove_running_time, bart_nli_running_time, sbert_kg_running_time]})\n",
        "\n",
        "#Print evaluation metrics for multiclass classification\n",
        "print(\"Evaluation Metrics for Zero-Shot Multiclass Classification:\")\n",
        "print(evaluation_metrics)\n",
        "\n",
        "#Evaluate performance of models for multilabel classification\n",
        "glove_precision, glove_recall, glove_f1, glove_accuracy, glove_hamming_loss, glove_running_time = evaluate_model_performance(glove_avg_classification, tweets, labels_binary, label_names, mlb)\n",
        "bert_cls_precision, bert_cls_recall, bert_cls_f1, bert_cls_accuracy, bert_cls_hamming_loss, bert_cls_running_time = evaluate_model_performance(bert_cls_classification, tweets, labels_binary, bert_cls_embeddings, mlb)\n",
        "bert_avg_precision, bert_avg_recall, bert_avg_f1, bert_avg_accuracy, bert_avg_hamming_loss, bert_avg_running_time = evaluate_model_performance(bert_avg_classification, tweets, labels_binary, bert_avg_embeddings, mlb)\n",
        "sbert_precision, sbert_recall, sbert_f1, sbert_accuracy, sbert_hamming_loss, sbert_running_time = evaluate_model_performance(sbert_classification, tweets, labels_binary, sbert_embeddings, mlb)\n",
        "sbert_glove_precision, sbert_glove_recall, sbert_glove_f1, sbert_glove_accuracy, sbert_glove_hamming_loss, sbert_glove_running_time = evaluate_model_performance(sbert_glove_classification, tweets, labels_binary, sbert_glove_embeddings, mlb)\n",
        "bart_nli_precision, bart_nli_recall, bart_nli_f1, bart_nli_accuracy, bart_nli_hamming_loss, bart_nli_running_time = evaluate_model_performance(bart_nli_classification, tweets, labels_binary, label_names, mlb)\n",
        "sbert_kg_precision, sbert_kg_recall, sbert_kg_f1, sbert_kg_accuracy, sbert_kg_hamming_loss, sbert_kg_running_time = evaluate_model_performance(sbert_kg_classification, tweets, labels_binary, sbert_kg_embeddings, mlb)\n",
        "\n",
        "#Create dataframe to store evaluation metrics\n",
        "evaluation_metrics = pd.DataFrame({'Model': ['GloVe', 'BERT-CLS', 'BERT-AVG', 'S-BERT', 'S-BERT-GloVe', 'BART-NLI', 'S-BERT-KG'],\n",
        "'Precision': [glove_precision, bert_cls_precision, bert_avg_precision, sbert_precision, sbert_glove_precision, bart_nli_precision, sbert_kg_precision],\n",
        "'Recall': [glove_recall, bert_cls_recall, bert_avg_recall, sbert_recall, sbert_glove_recall, bart_nli_recall, sbert_kg_recall],\n",
        "'F1 Score': [glove_f1, bert_cls_f1, bert_avg_f1, sbert_f1, sbert_glove_f1, bart_nli_f1, sbert_kg_f1],\n",
        "'Accuracy': [glove_accuracy, bert_cls_accuracy, bert_avg_accuracy, sbert_accuracy, sbert_glove_accuracy, bart_nli_accuracy, sbert_kg_accuracy],\n",
        "'Hamming Loss': [glove_hamming_loss, bert_cls_hamming_loss, bert_avg_hamming_loss, sbert_hamming_loss, sbert_glove_hamming_loss, bart_nli_hamming_loss, sbert_kg_hamming_loss],\n",
        "'Running Time (s)': [glove_running_time, bert_cls_running_time, bert_avg_running_time, sbert_running_time, sbert_glove_running_time, bart_nli_running_time, sbert_kg_running_time]})\n",
        "\n",
        "#Print evaluation metrics for multilabel classification\n",
        "print(\"Evaluation Metrics for Zero-Shot Multilabel Classification:\")\n",
        "print(evaluation_metrics)\n",
        "\n",
        "#Generate t-SNE visualization of sentence and label embeddings\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "\n",
        "#Obtain embeddings for GloVe-AVG model\n",
        "glove_avg_embeddings = glove_model.loc[tweets.index].values\n",
        "\n",
        "#Obtain embeddings for S-BERT model\n",
        "sbert_embeddings = sbert_model.encode(tweets)\n",
        "\n",
        "#Obtain embeddings for S-BERT-GloVe model\n",
        "sbert_glove_embeddings = sbert_glove_model.encode(tweets)\n",
        "\n",
        "#Obtain embeddings for S-BERT-KG model\n",
        "sbert_kg_embeddings = sbert_kg_model.encode(tweets)\n",
        "\n",
        "#Obtain t-SNE visualization for GloVe-AVG model\n",
        "glove_avg_tsne = tsne.fit_transform(glove_avg_embeddings)\n",
        "plt.scatter(glove_avg_tsne[:, 0], glove_avg_tsne[:, 1], c=labels_binary)\n",
        "plt.title(\"t-SNE Visualization of GloVe-AVG Embeddings\")\n",
        "plt.show()\n",
        "\n",
        "#Obtain t-SNE visualization for S-BERT model\n",
        "sbert_tsne = tsne.fit_transform(sbert_embeddings)\n",
        "plt.scatter(sbert_tsne[:, 0], sbert_tsne[:, 1], c=labels_binary)\n",
        "plt.title(\"t-SNE Visualization of S-BERT Embeddings\")\n",
        "plt.show()\n",
        "\n",
        "#Obtain t-SNE visualization for S-BERT-KG model\n",
        "sbert_kg_tsne = tsne.fit_transform(sbert_kg_embeddings)\n",
        "plt.scatter(sbert_kg_tsne[:, 0], sbert_kg_tsne[:, 1], c=labels_binary)\n",
        "plt.title(\"t-SNE Visualization of S-BERT-KG Embeddings\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9goOJYPwm7El"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}