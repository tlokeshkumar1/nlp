{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPAcVO4gf5za7X7iQASMG/b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tlokeshkumar1/nlp/blob/master/nlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrBaHfByscvb",
        "outputId": "db70c6bb-1ec0-4792-fe67-e4d4f6dfaa3d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (1.21.6)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (2022.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQe4-4W9skyW",
        "outputId": "c1574978-5d7a-41d2-8308-503d7c2acbc2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 895
        },
        "id": "QtL1aU_xsOEj",
        "outputId": "7db96950-6ab8-4655-c040-1cb2c2e6f8ae"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                           user_name               user_location  \\\n",
              "0                             ·èâ·é•‚òª’¨ÍÇÖœÆ                  astroworld   \n",
              "1                      Tom Basile üá∫üá∏                New York, NY   \n",
              "2                    Time4fisticuffs            Pewee Valley, KY   \n",
              "3                        ethel mertz        Stuck in the Middle    \n",
              "4                           DIPR-J&K           Jammu and Kashmir   \n",
              "5                   üéπ Franz Schubert                 –ù–æ–≤–æ—Ä–æÃÅ—Å—Å–∏—è   \n",
              "6                       hr bartender             Gainesville, FL   \n",
              "7                     Derbyshire LPC                         NaN   \n",
              "8                  Prathamesh Bendre                         NaN   \n",
              "9  Member of Christ üá®üá≥üá∫üá∏üáÆüá≥üáÆüá©üáßüá∑üá≥üá¨üáßüá©üá∑üá∫  üëáüèªlocation at link belowüëáüèª   \n",
              "\n",
              "                                    user_description         user_created  \\\n",
              "0  wednesday addams as a disney princess keepin i...  2017-05-26 05:46:42   \n",
              "1  Husband, Father, Columnist & Commentator. Auth...  2009-04-16 20:06:23   \n",
              "2  #Christian #Catholic #Conservative #Reagan #Re...  2009-02-28 18:57:41   \n",
              "3  #Browns #Indians #ClevelandProud #[]_[] #Cavs ...  2019-03-07 01:45:06   \n",
              "4  üñäÔ∏èOfficial Twitter handle of Department of Inf...  2017-02-12 06:45:15   \n",
              "5  üéº  #–ù–æ–≤–æ—Ä–æÃÅ—Å—Å–∏—è #Novorossiya #–æ—Å—Ç–∞–≤–∞–π—Å—è–¥–æ–º–∞ #S...  2018-03-19 16:29:52   \n",
              "6  Workplace tips and advice served up in a frien...  2008-08-12 18:19:49   \n",
              "7                                                NaN  2012-02-03 18:08:10   \n",
              "8   A poet, reiki practitioner and a student of law.  2015-04-25 08:15:41   \n",
              "9  Just as the body is one & has many members, & ...  2014-08-17 04:53:22   \n",
              "\n",
              "   user_followers  user_friends  user_favourites  user_verified  \\\n",
              "0             624           950            18775          False   \n",
              "1            2253          1677               24           True   \n",
              "2            9275          9525             7254          False   \n",
              "3             197           987             1488          False   \n",
              "4          101009           168              101          False   \n",
              "5            1180          1071             1287          False   \n",
              "6           79956         54810             3801          False   \n",
              "7             608           355               95          False   \n",
              "8              25            29               18          False   \n",
              "9           55201         34239            29802          False   \n",
              "\n",
              "                  date                                               text  \\\n",
              "0  2020-07-25 12:27:21  If I smelled the scent of hand sanitizers toda...   \n",
              "1  2020-07-25 12:27:17  Hey @Yankees @YankeesPR and @MLB - wouldn't it...   \n",
              "2  2020-07-25 12:27:14  @diane3443 @wdunlap @realDonaldTrump Trump nev...   \n",
              "3  2020-07-25 12:27:10  @brookbanktv The one gift #COVID19 has give me...   \n",
              "4  2020-07-25 12:27:08  25 July : Media Bulletin on Novel #CoronaVirus...   \n",
              "5  2020-07-25 12:27:06  #coronavirus #covid19 deaths continue to rise....   \n",
              "6  2020-07-25 12:27:03  How #COVID19 Will Change Work in General (and ...   \n",
              "7  2020-07-25 12:27:00  You now have to wear face coverings when out s...   \n",
              "8  2020-07-25 12:26:59  Praying for good health and recovery of @Chouh...   \n",
              "9  2020-07-25 12:26:54  POPE AS GOD - Prophet Sadhu Sundar Selvaraj. W...   \n",
              "\n",
              "                            hashtags               source  is_retweet  \n",
              "0                                NaN   Twitter for iPhone       False  \n",
              "1                                NaN  Twitter for Android       False  \n",
              "2                        ['COVID19']  Twitter for Android       False  \n",
              "3                        ['COVID19']   Twitter for iPhone       False  \n",
              "4  ['CoronaVirusUpdates', 'COVID19']  Twitter for Android       False  \n",
              "5         ['coronavirus', 'covid19']      Twitter Web App       False  \n",
              "6          ['COVID19', 'Recruiting']               Buffer       False  \n",
              "7                                NaN            TweetDeck       False  \n",
              "8       ['covid19', 'covidPositive']  Twitter for Android       False  \n",
              "9      ['HurricaneHanna', 'COVID19']   Twitter for iPhone       False  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cb60196e-c977-42a7-804f-41e47d4a6128\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_name</th>\n",
              "      <th>user_location</th>\n",
              "      <th>user_description</th>\n",
              "      <th>user_created</th>\n",
              "      <th>user_followers</th>\n",
              "      <th>user_friends</th>\n",
              "      <th>user_favourites</th>\n",
              "      <th>user_verified</th>\n",
              "      <th>date</th>\n",
              "      <th>text</th>\n",
              "      <th>hashtags</th>\n",
              "      <th>source</th>\n",
              "      <th>is_retweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>·èâ·é•‚òª’¨ÍÇÖœÆ</td>\n",
              "      <td>astroworld</td>\n",
              "      <td>wednesday addams as a disney princess keepin i...</td>\n",
              "      <td>2017-05-26 05:46:42</td>\n",
              "      <td>624</td>\n",
              "      <td>950</td>\n",
              "      <td>18775</td>\n",
              "      <td>False</td>\n",
              "      <td>2020-07-25 12:27:21</td>\n",
              "      <td>If I smelled the scent of hand sanitizers toda...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Twitter for iPhone</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Tom Basile üá∫üá∏</td>\n",
              "      <td>New York, NY</td>\n",
              "      <td>Husband, Father, Columnist &amp; Commentator. Auth...</td>\n",
              "      <td>2009-04-16 20:06:23</td>\n",
              "      <td>2253</td>\n",
              "      <td>1677</td>\n",
              "      <td>24</td>\n",
              "      <td>True</td>\n",
              "      <td>2020-07-25 12:27:17</td>\n",
              "      <td>Hey @Yankees @YankeesPR and @MLB - wouldn't it...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Twitter for Android</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Time4fisticuffs</td>\n",
              "      <td>Pewee Valley, KY</td>\n",
              "      <td>#Christian #Catholic #Conservative #Reagan #Re...</td>\n",
              "      <td>2009-02-28 18:57:41</td>\n",
              "      <td>9275</td>\n",
              "      <td>9525</td>\n",
              "      <td>7254</td>\n",
              "      <td>False</td>\n",
              "      <td>2020-07-25 12:27:14</td>\n",
              "      <td>@diane3443 @wdunlap @realDonaldTrump Trump nev...</td>\n",
              "      <td>['COVID19']</td>\n",
              "      <td>Twitter for Android</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ethel mertz</td>\n",
              "      <td>Stuck in the Middle</td>\n",
              "      <td>#Browns #Indians #ClevelandProud #[]_[] #Cavs ...</td>\n",
              "      <td>2019-03-07 01:45:06</td>\n",
              "      <td>197</td>\n",
              "      <td>987</td>\n",
              "      <td>1488</td>\n",
              "      <td>False</td>\n",
              "      <td>2020-07-25 12:27:10</td>\n",
              "      <td>@brookbanktv The one gift #COVID19 has give me...</td>\n",
              "      <td>['COVID19']</td>\n",
              "      <td>Twitter for iPhone</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>DIPR-J&amp;K</td>\n",
              "      <td>Jammu and Kashmir</td>\n",
              "      <td>üñäÔ∏èOfficial Twitter handle of Department of Inf...</td>\n",
              "      <td>2017-02-12 06:45:15</td>\n",
              "      <td>101009</td>\n",
              "      <td>168</td>\n",
              "      <td>101</td>\n",
              "      <td>False</td>\n",
              "      <td>2020-07-25 12:27:08</td>\n",
              "      <td>25 July : Media Bulletin on Novel #CoronaVirus...</td>\n",
              "      <td>['CoronaVirusUpdates', 'COVID19']</td>\n",
              "      <td>Twitter for Android</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>üéπ Franz Schubert</td>\n",
              "      <td>–ù–æ–≤–æ—Ä–æÃÅ—Å—Å–∏—è</td>\n",
              "      <td>üéº  #–ù–æ–≤–æ—Ä–æÃÅ—Å—Å–∏—è #Novorossiya #–æ—Å—Ç–∞–≤–∞–π—Å—è–¥–æ–º–∞ #S...</td>\n",
              "      <td>2018-03-19 16:29:52</td>\n",
              "      <td>1180</td>\n",
              "      <td>1071</td>\n",
              "      <td>1287</td>\n",
              "      <td>False</td>\n",
              "      <td>2020-07-25 12:27:06</td>\n",
              "      <td>#coronavirus #covid19 deaths continue to rise....</td>\n",
              "      <td>['coronavirus', 'covid19']</td>\n",
              "      <td>Twitter Web App</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>hr bartender</td>\n",
              "      <td>Gainesville, FL</td>\n",
              "      <td>Workplace tips and advice served up in a frien...</td>\n",
              "      <td>2008-08-12 18:19:49</td>\n",
              "      <td>79956</td>\n",
              "      <td>54810</td>\n",
              "      <td>3801</td>\n",
              "      <td>False</td>\n",
              "      <td>2020-07-25 12:27:03</td>\n",
              "      <td>How #COVID19 Will Change Work in General (and ...</td>\n",
              "      <td>['COVID19', 'Recruiting']</td>\n",
              "      <td>Buffer</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Derbyshire LPC</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2012-02-03 18:08:10</td>\n",
              "      <td>608</td>\n",
              "      <td>355</td>\n",
              "      <td>95</td>\n",
              "      <td>False</td>\n",
              "      <td>2020-07-25 12:27:00</td>\n",
              "      <td>You now have to wear face coverings when out s...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>TweetDeck</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Prathamesh Bendre</td>\n",
              "      <td>NaN</td>\n",
              "      <td>A poet, reiki practitioner and a student of law.</td>\n",
              "      <td>2015-04-25 08:15:41</td>\n",
              "      <td>25</td>\n",
              "      <td>29</td>\n",
              "      <td>18</td>\n",
              "      <td>False</td>\n",
              "      <td>2020-07-25 12:26:59</td>\n",
              "      <td>Praying for good health and recovery of @Chouh...</td>\n",
              "      <td>['covid19', 'covidPositive']</td>\n",
              "      <td>Twitter for Android</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Member of Christ üá®üá≥üá∫üá∏üáÆüá≥üáÆüá©üáßüá∑üá≥üá¨üáßüá©üá∑üá∫</td>\n",
              "      <td>üëáüèªlocation at link belowüëáüèª</td>\n",
              "      <td>Just as the body is one &amp; has many members, &amp; ...</td>\n",
              "      <td>2014-08-17 04:53:22</td>\n",
              "      <td>55201</td>\n",
              "      <td>34239</td>\n",
              "      <td>29802</td>\n",
              "      <td>False</td>\n",
              "      <td>2020-07-25 12:26:54</td>\n",
              "      <td>POPE AS GOD - Prophet Sadhu Sundar Selvaraj. W...</td>\n",
              "      <td>['HurricaneHanna', 'COVID19']</td>\n",
              "      <td>Twitter for iPhone</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cb60196e-c977-42a7-804f-41e47d4a6128')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-cb60196e-c977-42a7-804f-41e47d4a6128 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-cb60196e-c977-42a7-804f-41e47d4a6128');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "import pandas as pd\n",
        "RESULTS_DIR = 'drive/MyDrive'\n",
        "covid19_tweets = pd.read_csv(RESULTS_DIR +'/covid19_tweets.csv')\n",
        "covid19_tweets.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import string\n",
        "from pathlib import Path\n",
        "RESULTS_DIR = 'drive/MyDrive'\n",
        "covid19_tweets = pd.read_csv(RESULTS_DIR +'/covid19_tweets.csv')\n",
        "\n",
        "\n",
        "# Concatenate the papers from different sources into one corpus dataframe.\n",
        "corpus_df = pd.concat([covid19_tweets])\n",
        "\n",
        "def clean_sentence(sentence):\n",
        "    \"\"\"\n",
        "    Cleans an individual sentence.\n",
        "    \n",
        "    Args:\n",
        "        sentence: An individual sentence from the corpus.\n",
        "    Returns:\n",
        "        clean_sentence: A cleaned sentence with unwanted linespaces removed.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Remove titles before linespaces\n",
        "    try:\n",
        "        clean_sentence = re.findall('\\n\\n(.*)', sentence)[-1]\n",
        "    except IndexError:\n",
        "        clean_sentence = sentence\n",
        "    \n",
        "    return clean_sentence\n",
        "\n",
        "def clean_sentences(sentences):\n",
        "    \"\"\"\n",
        "    Cleans each sentence in a list of sentences, corresponding to one article.\n",
        "    \n",
        "    Args:\n",
        "        sentences: A list of sentences from an article (paper).\n",
        "    Returns:\n",
        "        cleaned_sentences_no_punctuation: A list of cleaned sentences, with \"sentences\" that only contain punctuation removed.\n",
        "    \"\"\"\n",
        "    \n",
        "    cleaned_sentences = [clean_sentence(sentence) for sentence in sentences]\n",
        "    \n",
        "    # Remove sentences with only punctuation\n",
        "    cleaned_sentences_no_punctuation = [sentence for sentence in cleaned_sentences if sentence not in string.punctuation]\n",
        "    \n",
        "    return cleaned_sentences_no_punctuation\n",
        "    \n",
        "    \n",
        "# Retain the paper ID throughout the analysis.\n",
        "# The paper ID can be re-used to refer back to the relevant paper.\n",
        "corpus = corpus_df[['user_name', 'text']].values\n",
        "corpus = [(user_name, document) for user_name, document in corpus if type(document)==str]\n",
        "\n",
        "# Split into sentences for each article in the corpus\n",
        "documents = [(user_name, nltk.sent_tokenize(document)) for user_name, document in corpus]\n",
        "\n",
        "# Clean each sentence\n",
        "documents_clean = [(user_name, clean_sentences(document)) for user_name, document in documents]\n",
        "        \n",
        "# Flatten documents into a list of sentences\n",
        "sentences_clean = [item for sublist in documents_clean for item in sublist]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAZ8wtD-tGaU",
        "outputId": "ff2bc409-146b-4a38-e946-a9f58bd575f9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install fasttext"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bM-H4KgAtXru",
        "outputId": "b722dc26-c9b5-45bd-d720-82b5eb57333a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 68 kB 3.3 MB/s \n",
            "\u001b[?25hCollecting pybind11>=2.2\n",
            "  Using cached pybind11-2.10.2-py3-none-any.whl (222 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from fasttext) (57.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from fasttext) (1.21.6)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp38-cp38-linux_x86_64.whl size=3133443 sha256=002ec8a0ba8be5a9fdd9f79557cd7c44a606e73d983406f58eb6d6098413b16c\n",
            "  Stored in directory: /root/.cache/pip/wheels/93/61/2a/c54711a91c418ba06ba195b1d78ff24fcaad8592f2a694ac94\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.2 pybind11-2.10.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We will use FastText for pre-trained word-embeddings.\n",
        "# As training word embeddings from the CORD-19 corpus and finding words with similar embeddings will be computationally expensive.\n",
        "import fasttext\n",
        "\n",
        "keywords = ['quarantine', 'stay-at-home', 'shelter-in-place', 'shutdown', 'lockdown']\n",
        "\n",
        "fasttext.util.download_model('en', if_exists='ignore')  # English\n",
        "ft = fasttext.load_model('cc.en.300.bin')\n",
        "\n",
        "# Word embeddings can be used to return words that are close within the vector space\n",
        "print(keywords[0])\n",
        "ft.get_nearest_neighbors(keywords[0])\n",
        "\n",
        "keywords_extended = []\n",
        "for keyword in keywords:\n",
        "    keywords_extended.append(keyword)\n",
        "    \n",
        "    nearest_neighbours = [word.lower() for similarity, word in ft.get_nearest_neighbors(keyword)]\n",
        "    keywords_extended += nearest_neighbours\n",
        "    \n",
        "keywords_set = set(keywords_extended)\n",
        "\n",
        "# Manually remove irrelevant words\n",
        "keywords_set = keywords_set - {'active-shooter', 'boil-water', 'high-security', 'bio-security', 'shutdown.the', 'pre-evacuation', 'biosecurity', 'evacuate','evacuation','evacuations'\n",
        "                              'sahm','stay-at','stay-at-home-dad','stay-at-home-mom','stay-at-home-moms','stay-at-home-mother','stay-at-home-mum', 'sahm'}\n",
        "\n",
        "# As the keywords returned are only unigrams (with hyphens) we can also include ngrams which may be found in the corpus.\n",
        "keywords_with_spaces = set([word.replace('-', ' ') for word in list(keywords_set)])\n",
        "keywords_set = keywords_set.union(keywords_with_spaces)\n",
        "\n",
        "print(keywords_set)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "xIPWrx_0tSJ7",
        "outputId": "9181e033-bc26-4c23-9ec1-4f924b5f3733"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-66758e11cb94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mkeywords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'quarantine'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'stay-at-home'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shelter-in-place'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shutdown'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lockdown'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mfasttext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mif_exists\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# English\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfasttext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cc.en.300.bin'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'fasttext' has no attribute 'util'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4-Zi6trtstZ",
        "outputId": "767d1d40-3da7-403f-cd8a-3ff7fa77d895"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 85 kB 2.8 MB/s \n",
            "\u001b[?25hCollecting transformers<5.0.0,>=4.6.0\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5.8 MB 33.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (4.64.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (1.13.0+cu116)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (0.14.0+cu116)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (1.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (1.7.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (3.7)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.3 MB 50.8 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub>=0.4.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 182 kB 66.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.4.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.8.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers) (3.0.9)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.6.2)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7.6 MB 49.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125938 sha256=f51358cc09f8ab2ad4ce1e8ac13b5a11a6cb3bb196b0627fbd9f1e195137a187\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/6f/8c/d88aec621f3f542d26fac0342bef5e693335d125f4e54aeffe\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers, sentencepiece, sentence-transformers\n",
            "Successfully installed huggingface-hub-0.11.1 sentence-transformers-2.2.2 sentencepiece-0.1.97 tokenizers-0.13.2 transformers-4.25.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUWYcouduCsU",
        "outputId": "1df27bcd-b42c-4e56-9e48-6399833594e4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (1.0.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.7.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.21.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install datasets transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iisn5M0QuGpZ",
        "outputId": "8ac54f48-c476-432c-c9e6-02f0bc3ad241"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.8.0-py3-none-any.whl (452 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 452 kB 4.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.25.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from datasets) (1.21.6)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (2022.11.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.11.1)\n",
            "Requirement already satisfied: dill<0.3.7 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.3.6)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (4.64.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets) (3.8.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets) (1.3.5)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 132 kB 66.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets) (21.3)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 212 kB 68.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (22.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (6.0.3)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (2.1.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.8.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.8.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->datasets) (3.0.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 127 kB 75.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2022.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: urllib3, xxhash, responses, multiprocess, datasets\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed datasets-2.8.0 multiprocess-0.70.14 responses-0.18.0 urllib3-1.25.11 xxhash-3.1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "urllib3"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using DistilBERT from Hugging Face for sentence embeddings\n",
        "# https://medium.com/huggingface/distilbert-8cf3380435b5\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
        "embeddings = model.encode(relevant_sentences, show_progress_bar=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "id": "0GrN7MVtt0et",
        "outputId": "5d8d6e48-a9b6-4f0e-9361-bcb8270564bf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-86790de25c2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'distilbert-base-nli-mean-tokens'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelevant_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'relevant_sentences' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install umap-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzrEW61FuUZz",
        "outputId": "31760119-dcd1-4565-a56e-44097e3f1682"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting umap-learn\n",
            "  Downloading umap-learn-0.5.3.tar.gz (88 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 88 kB 2.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from umap-learn) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.8/dist-packages (from umap-learn) (1.0.2)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.8/dist-packages (from umap-learn) (1.7.3)\n",
            "Requirement already satisfied: numba>=0.49 in /usr/local/lib/python3.8/dist-packages (from umap-learn) (0.56.4)\n",
            "Collecting pynndescent>=0.5\n",
            "  Downloading pynndescent-0.5.8.tar.gz (1.1 MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.1 MB 33.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from umap-learn) (4.64.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from numba>=0.49->umap-learn) (57.4.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.8/dist-packages (from numba>=0.49->umap-learn) (5.1.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.8/dist-packages (from numba>=0.49->umap-learn) (0.39.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from pynndescent>=0.5->umap-learn) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.22->umap-learn) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata->numba>=0.49->umap-learn) (3.11.0)\n",
            "Building wheels for collected packages: umap-learn, pynndescent\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for umap-learn: filename=umap_learn-0.5.3-py3-none-any.whl size=82829 sha256=bb5487d9a91d0c060d52385cc86b733513eb06de8052118cf173aeb25eb40168\n",
            "  Stored in directory: /root/.cache/pip/wheels/a9/3a/67/06a8950e053725912e6a8c42c4a3a241410f6487b8402542ea\n",
            "  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pynndescent: filename=pynndescent-0.5.8-py3-none-any.whl size=55513 sha256=7de3431ae5086f7bd4779d9556130567cb4b14fbeed14dfc35c88ce07e60bf57\n",
            "  Stored in directory: /root/.cache/pip/wheels/1c/63/3a/29954bca1a27ba100ed8c27973a78cb71b43dc67aed62e80c3\n",
            "Successfully built umap-learn pynndescent\n",
            "Installing collected packages: pynndescent, umap-learn\n",
            "Successfully installed pynndescent-0.5.8 umap-learn-0.5.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install matplotlib-venn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-E2CAsmucVM",
        "outputId": "b2c37445-ccd8-43de-8677-668c8baa0130"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: matplotlib-venn in /usr/local/lib/python3.8/dist-packages (0.11.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from matplotlib-venn) (1.21.6)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from matplotlib-venn) (3.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from matplotlib-venn) (1.7.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->matplotlib-venn) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->matplotlib-venn) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->matplotlib-venn) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->matplotlib-venn) (1.4.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib->matplotlib-venn) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import umap.umap_ as umap\n",
        "from keras.layers import Embedding\n",
        "umap_embeddings = umap.UMAP(n_neighbors=15, \n",
        "                            n_components=5, \n",
        "                            metric='cosine').fit_transform(embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "id": "f_zcZ69huhI8",
        "outputId": "5b8aa1f8-9e1a-4328-f357-247047406b2d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-e10025abc048>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m umap_embeddings = umap.UMAP(n_neighbors=15, \n\u001b[1;32m      4\u001b[0m                             \u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                             metric='cosine').fit_transform(embeddings)\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'embeddings' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install hdbscan"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1QOHxLKuxV9",
        "outputId": "b450533b-87f9-45d4-a15b-f586a7132e61"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting hdbscan\n",
            "  Downloading hdbscan-0.8.29.tar.gz (5.2 MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5.2 MB 4.8 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cython>=0.27 in /usr/local/lib/python3.8/dist-packages (from hdbscan) (0.29.32)\n",
            "Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.8/dist-packages (from hdbscan) (1.0.2)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.8/dist-packages (from hdbscan) (1.2.0)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.8/dist-packages (from hdbscan) (1.7.3)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from hdbscan) (1.21.6)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.20->hdbscan) (3.1.0)\n",
            "Building wheels for collected packages: hdbscan\n",
            "  Building wheel for hdbscan (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hdbscan: filename=hdbscan-0.8.29-cp38-cp38-linux_x86_64.whl size=2700863 sha256=2c1002aac62a6f83c24a140b9d0b0c1e86d304d8425ee21761d5f0922adad741\n",
            "  Stored in directory: /root/.cache/pip/wheels/76/06/48/527e038689c581cc9e519c73840efdc7473805149e55bd7ffd\n",
            "Successfully built hdbscan\n",
            "Installing collected packages: hdbscan\n",
            "Successfully installed hdbscan-0.8.29\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import hdbscan\n",
        "\n",
        "cluster = hdbscan.HDBSCAN(min_cluster_size=20,\n",
        "                          metric='euclidean',                      \n",
        "                          cluster_selection_method='eom').fit(umap_embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "id": "BdbYa0YnvIOo",
        "outputId": "9051384d-ae47-4d3d-8555-b1dd8fbc1c35"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-8420de7a8c66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m cluster = hdbscan.HDBSCAN(min_cluster_size=20,\n\u001b[1;32m      4\u001b[0m                           \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'euclidean'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                           cluster_selection_method='eom').fit(umap_embeddings)\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'umap_embeddings' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Names for each topic were manually created by looking through examples in each topic and the most relevant words\n",
        "topic_labels_dict = {\n",
        " -1: 'outliers',\n",
        "  0: 'electrical systems',\n",
        "  1: 'cellular shutdown',\n",
        "  2: 'australia',\n",
        "  3: 'korea',\n",
        "  4: 'links',\n",
        "  5: 'cities in lockdown',\n",
        "  6: 'canada',\n",
        "  7: 'historical quarantines',\n",
        "  8: 'sars',\n",
        "  9: 'thermoscanners',\n",
        "  10: 'animals',\n",
        "  11: 'copyright',\n",
        "  12: 'quarantine measures',\n",
        "  13: 'authorities',\n",
        "  14: 'japan',\n",
        "  15: 'studies on quarantined individuals',\n",
        "  16: 'lockdown & quarantine consequences',\n",
        "  17: 'increasing cases',\n",
        "  18: 'lockdown effect on transmission rate',\n",
        "  19: 'lockdown reducing infections',\n",
        "  20: 'quarantine length',\n",
        "  21: 'wuhan',\n",
        "  22: 'china',\n",
        "  23: 'early 2020',\n",
        "  24: 'quantative metrics',\n",
        "  25: 'other epidemics'\n",
        "}\n",
        "\n",
        "def get_centroid(arr):\n",
        "  \"\"\"\n",
        "  Retrieves the centroid of a cluster, given the set of coordinates of points within the cluster.\n",
        "  \n",
        "  Args:\n",
        "      arr: Array containing set of coordinates of points within a cluster.\n",
        "  Returns:\n",
        "      x_centre, y_centre: Coordinates of the centroid of the cluster.\n",
        "  \"\"\"\n",
        "    length = arr.shape[0]\n",
        "    sum_x = np.sum(arr[:, 0])\n",
        "    sum_y = np.sum(arr[:, 1])\n",
        "    x_centre, y_centre = sum_x/length, sum_y/length\n",
        "    return x_centre, y_centre\n",
        "  \n",
        "# Visualize clusters\n",
        "fig, ax = plt.subplots(figsize=(20, 10), dpi=200)\n",
        "outliers = result.loc[result.labels == -1, :]\n",
        "clustered = result.loc[result.labels != -1, :]\n",
        "plt.scatter(outliers.x, outliers.y, color='#BDBDBD', s=0.5)\n",
        "plt.scatter(clustered.x, clustered.y, c=clustered.labels, s=0.5, cmap='hsv_r')\n",
        "plt.colorbar()\n",
        "\n",
        "# Plot topic names\n",
        "for label in result['labels']:\n",
        "    # skip outliers\n",
        "    if label == -1:\n",
        "        pass\n",
        "    else:\n",
        "        label_name = topic_labels_dict[label]\n",
        "        coords = result[result['labels']==label][['x', 'y']].values\n",
        "        x_pos, y_pos = get_centroid(coords)\n",
        "        plt.text(x_pos, y_pos, label_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "id": "WRv8pbF9vKdf",
        "outputId": "3f2ed681-526f-4d8c-839d-7237c75ea837"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-6019dc3939a7>\"\u001b[0;36m, line \u001b[0;32m41\u001b[0m\n\u001b[0;31m    length = arr.shape[0]\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ]
    }
  ]
}