{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN6KpGodwqRp4knqBpQcqIS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tlokeshkumar1/nlp/blob/master/Untitled3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0dfnflsj_s1",
        "outputId": "96001b1e-6886-4d57-cc4a-eaf07d5c25fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read in the CSV files\n",
        "RESULTS_DIR = 'drive/MyDrive'\n",
        "df1 = pd.read_csv(RESULTS_DIR + '/D11.csv', encoding='latin1', engine='python')\n",
        "df2 = pd.read_csv(RESULTS_DIR + '/D2.csv', encoding='latin1', engine='python')\n",
        "\n",
        "\n",
        "# Merge the dataframes\n",
        "merged_df = pd.concat([df1, df2], axis=0)\n",
        "\n",
        "# Create a new column called 'category' and initialize it with 'None'\n",
        "merged_df['category'] = None\n",
        "\n",
        "# Create a list of adive,china,mask,news,transporation,usa and vaccine\n",
        "Advice = [\"['Stay at home']\",\"['wash hands']\",\"['wear mask']\",\"['social distancing']\"]\n",
        "China = [\"['Wuhan']\",\"['China Coronavirus Updates']\",\"['China news']\",\"['other tweets related to China']\"]\n",
        "Mask = [\"['Mask shortage']\",\"['wear mask']\",\"['mask types']\",\"['N50']\",\"['N95']\",\"['3M8210']\",\"['3M9001']\",\"['3M9322']\",\"['3M9501']\"]\n",
        "News = [\"['Coronavirus updates']\",\"['news']\",\"['rules']\"]\n",
        "Transportation = [\"['Flights']\",\"['traffic']\",\"['traveling']\"]\n",
        "USA = [\"['U.S. Coronavirus Updates']\",\"['COVID19']\",\"['U.S. news']\",\"['United States']\",\"['US']\",\"['USA']\"]\n",
        "Vaccine = [\"['Vaccine news']\",\"['vaccine progress']\",\"['vaccine injection']\"]\n",
        "\n",
        "# Iterate over the rows of the dataframe\n",
        "for index, row in merged_df.iterrows():\n",
        "    # Check if the hashtags column contains  'Stay at home','wash hands','wear mask' or 'social distancing'\n",
        "    if row['hashtags'] in Advice:\n",
        "        # If it does, set the category to 'Advice'\n",
        "        merged_df.at[index, 'category'] = 'Advice'\n",
        "    elif row['hashtags'] in China:\n",
        "        # If it does, set the category to 'China'\n",
        "        merged_df.at[index, 'category'] = 'China'\n",
        "    elif row['hashtags'] in Mask:\n",
        "        # If it does, set the category to 'Mask'\n",
        "        merged_df.at[index, 'category'] = 'Mask'\n",
        "    elif row['hashtags'] in News:\n",
        "        # If it does, set the category to 'News'\n",
        "        merged_df.at[index, 'category'] = 'News'\n",
        "    elif row['hashtags'] in Transportation:\n",
        "        # If it does, set the category to 'Transportation'\n",
        "        merged_df.at[index, 'category'] = 'Transportation'\n",
        "    elif row['hashtags'] in USA:\n",
        "        # If it does, set the category to 'USA'\n",
        "        merged_df.at[index, 'category'] = 'USA'\n",
        "    elif row['hashtags'] in Vaccine:\n",
        "        # If it does, set the category to 'Vaccine'\n",
        "        merged_df.at[index, 'category'] = 'Vaccine'\n",
        "    \n",
        "# Print the updated dataframe\n",
        "print(merged_df[\"category\"].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VG7Xg6DGkGoC",
        "outputId": "877b5f34-b6bb-4b9c-8f88-63643c4c3daf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mask              469\n",
            "Advice            311\n",
            "USA               224\n",
            "News              216\n",
            "Vaccine           134\n",
            "Transportation    132\n",
            "China             131\n",
            "Name: category, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04Vq1HQbkpCT",
        "outputId": "bd00b07a-e3ff-4675-cfc7-0372dd13c62d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 KB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting transformers<5.0.0,>=4.6.0\n",
            "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (4.64.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (1.13.1+cu116)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (0.14.1+cu116)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (1.22.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (1.2.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (1.10.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (3.7)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub>=0.4.0\n",
            "  Downloading huggingface_hub-0.12.1-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.9.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.25.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.6.2)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk->sentence-transformers) (8.1.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision->sentence-transformers) (8.4.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.14)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.12.7)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125938 sha256=e3a9e195c0eb307000e8332e381749946267fa7f00a8e3d4f26e2ce57240a217\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/6f/8c/d88aec621f3f542d26fac0342bef5e693335d125f4e54aeffe\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: tokenizers, sentencepiece, huggingface-hub, transformers, sentence-transformers\n",
            "Successfully installed huggingface-hub-0.12.1 sentence-transformers-2.2.2 sentencepiece-0.1.97 tokenizers-0.13.2 transformers-4.26.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Read the merged DataFrame containing the tweets and categories\n",
        "merged_df = pd.concat([df1, df2], axis=0)\n",
        "\n",
        "# Load the pre-trained S-BERT model\n",
        "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "\n",
        "# Convert the tweets into sentence embeddings\n",
        "tweet_embeddings = model.encode(merged_df['text'].values)\n",
        "\n",
        "# Define the possible label names\n",
        "label_names = ['Vaccine','USA','Transportation','News','Mask','China','Advice']\n",
        "\n",
        "# Convert the label names into label embeddings\n",
        "label_embeddings = model.encode(label_names)\n",
        "\n",
        "# Calculate the cosine similarities between each tweet embedding and each label embedding\n",
        "cos_similarities = cosine_similarity(tweet_embeddings, label_embeddings)\n",
        "\n",
        "# Assign the label with the highest cosine similarity to each tweet\n",
        "predicted_labels = [label_names[i] for i in cos_similarities.argmax(axis=1)]\n",
        "\n",
        "# Add the predicted labels to the merged DataFrame\n",
        "merged_df['predicted_label'] = predicted_labels\n",
        "\n",
        "# Print the resulting DataFrame\n",
        "print(merged_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JaSb7weYkhLx",
        "outputId": "82c743c5-3018-4895-b08a-8b8cbd58c840"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                user_name                  user_location  \\\n",
            "0     Tom Basile ðºð¸                   New York, NY   \n",
            "1         Time4fisticuffs               Pewee Valley, KY   \n",
            "2             ethel mertz           Stuck in the Middle    \n",
            "3                DIPR-J&K              Jammu and Kashmir   \n",
            "4     ð¹ Franz Schubert         ÐÐ¾Ð²Ð¾ÑÐ¾ÌÑÑÐ¸Ñ   \n",
            "...                   ...                            ...   \n",
            "2004     Bruno A. Bonechi                            NaN   \n",
            "2005           UAE Forsan  United Arab Emirates ð¦ðª   \n",
            "2006          TheArgus.in             Bhubaneswar, India   \n",
            "2007      MSF East Africa                 Nairobi, Kenya   \n",
            "2008           Lisa White                            NaN   \n",
            "\n",
            "                                       user_description      user_created  \\\n",
            "0     Husband, Father, Columnist & Commentator. Auth...  16-04-2009 20:06   \n",
            "1     #Christian #Catholic #Conservative #Reagan #Re...  28-02-2009 18:57   \n",
            "2     #Browns #Indians #ClevelandProud #[]_[] #Cavs ...  07-03-2019 01:45   \n",
            "3     ðï¸Official Twitter handle of Department o...  12-02-2017 06:45   \n",
            "4     ð¼  #ÐÐ¾Ð²Ð¾ÑÐ¾ÌÑÑÐ¸Ñ #Novorossiya #Ð¾...  19-03-2018 16:29   \n",
            "...                                                 ...               ...   \n",
            "2004  Partner @ Strategy& PwC | Curious Involved Com...  04-02-2012 16:51   \n",
            "2005  #UAE ð¦ðª Forsan is your comprehensive sou...  24-11-2012 21:19   \n",
            "2006                             24x7 Web  News Channel  13-01-2018 06:45   \n",
            "2007  MÃ©decins Sans FrontiÃ¨res / Doctors Without B...  13-05-2013 08:35   \n",
            "2008  Click the link below to add me on Snapchat: Li...  25-07-2020 06:11   \n",
            "\n",
            "      user_followers  user_friends  user_favourites  user_verified  \\\n",
            "0               2253          1677               24           True   \n",
            "1               9275          9525             7254          False   \n",
            "2                197           987             1488          False   \n",
            "3             101009           168              101          False   \n",
            "4               1180          1071             1287          False   \n",
            "...              ...           ...              ...            ...   \n",
            "2004            2727          2225               56          False   \n",
            "2005           13083             3              366           True   \n",
            "2006            3362           483              251          False   \n",
            "2007            8344          1089              902           True   \n",
            "2008              19             0                0          False   \n",
            "\n",
            "                  date                                               text  \\\n",
            "0     25-07-2020 12:27  Hey @Yankees @YankeesPR and @MLB - wouldn't it...   \n",
            "1     25-07-2020 12:27  @diane3443 @wdunlap @realDonaldTrump Trump nev...   \n",
            "2     25-07-2020 12:27  @brookbanktv The one gift #COVID19 has give me...   \n",
            "3     25-07-2020 12:27  25 July : Media Bulletin on Novel #CoronaVirus...   \n",
            "4     25-07-2020 12:27  #coronavirus #covid19 deaths continue to rise....   \n",
            "...                ...                                                ...   \n",
            "2004  25-07-2020 11:20  There is an urgent need for global stakeholder...   \n",
            "2005  25-07-2020 11:20  A community group consisting of 10 Indian ladi...   \n",
            "2006  25-07-2020 11:20  Now, 11 #COVID19 patients admitted to ICU and ...   \n",
            "2007  25-07-2020 11:20  For many #refugees, humanitarian services are ...   \n",
            "2008  25-07-2020 11:20  Dm to buy nudes videos calls and custom videos...   \n",
            "\n",
            "                   source  is_retweet                        hashtags  \\\n",
            "0     Twitter for Android       False              ['Vaccine news']     \n",
            "1     Twitter for Android       False           ['vaccine injection']   \n",
            "2      Twitter for iPhone       False                         ['USA']   \n",
            "3     Twitter for Android       False               ['United States']   \n",
            "4         Twitter Web App       False                         ['N50']   \n",
            "...                   ...         ...                             ...   \n",
            "2004       Hootsuite Inc.       False                     ['Wuhan']     \n",
            "2005       Hootsuite Inc.       False           ['vaccine injection']   \n",
            "2006      Twitter Web App       False                         ['N50']   \n",
            "2007               Buffer       False  ['Mask shortage']                \n",
            "2008            TweetDeck       False              ['Vaccine news']     \n",
            "\n",
            "     predicted_label  \n",
            "0              China  \n",
            "1                USA  \n",
            "2             Advice  \n",
            "3            Vaccine  \n",
            "4               News  \n",
            "...              ...  \n",
            "2004            News  \n",
            "2005         Vaccine  \n",
            "2006         Vaccine  \n",
            "2007         Vaccine  \n",
            "2008         Vaccine  \n",
            "\n",
            "[3778 rows x 14 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load the pre-trained S-BERT model\n",
        "tokenizer = AutoTokenizer.from_pretrained('/content/drive/MyDrive/sentence-transformers/sentence-transformers-all-distilroberta-v1')\n",
        "model = AutoModel.from_pretrained('/content/drive/MyDrive/sentence-transformers/sentence-transformers-all-distilroberta-v1')\n",
        "\n",
        "def load_conceptnet_graph():\n",
        "    # Load the ConceptNet data into a pandas DataFrame\n",
        "    conceptnet_data = pd.read_csv('/content/drive/MyDrive/conceptnet-assertions-5.7.0.csv', sep='\\t', header=None, names=['relation', 'start', 'end', 'weight'])\n",
        "\n",
        "    # Create a NetworkX graph\n",
        "    conceptnet_graph = nx.DiGraph()\n",
        "\n",
        "    # Iterate over each row of the DataFrame and add edges to the graph\n",
        "    for i, row in conceptnet_data.iterrows():\n",
        "        relation = row['relation']\n",
        "        start = row['start']\n",
        "        end = row['end']\n",
        "        weight = row['weight']\n",
        "\n",
        "        # Add the edge to the graph\n",
        "        conceptnet_graph.add_edge(start, end, weight=weight, relation=relation)\n",
        "\n",
        "    return conceptnet_graph\n",
        "\n",
        "# Load the ConceptNet knowledge graph\n",
        "# (You will need to download and preprocess the ConceptNet graph)\n",
        "graph = load_conceptnet_graph()\n",
        "\n",
        "def retrofit_graph(graph, model, num_iterations=10):\n",
        "    # Get the S-BERT embeddings for each node in the graph\n",
        "    node_embeddings = {}\n",
        "    for node in graph.nodes():\n",
        "        node_embeddings[node] = model.encode(node).squeeze().detach().numpy()\n",
        "    \n",
        "    # Initialize the new embeddings with the S-BERT embeddings\n",
        "    new_node_embeddings = node_embeddings.copy()\n",
        "    \n",
        "    # Perform the retrofitting iterations\n",
        "    for i in tqdm(range(num_iterations)):\n",
        "        for node in graph.nodes():\n",
        "            # Get the neighbors of the node in the graph\n",
        "            neighbors = list(graph.neighbors(node))\n",
        "            num_neighbors = len(neighbors)\n",
        "            \n",
        "            # Compute the weighted sum of the node's neighbors' embeddings\n",
        "            neighbor_embeddings_sum = np.zeros_like(node_embeddings[node])\n",
        "            for neighbor in neighbors:\n",
        "                weight = graph[node][neighbor]['weight']\n",
        "                neighbor_embeddings_sum += weight * node_embeddings[neighbor]\n",
        "            \n",
        "            # Compute the new embedding for the node by averaging the S-BERT embedding and the neighbor embeddings\n",
        "            new_node_embeddings[node] = 0.5 * (node_embeddings[node] + (neighbor_embeddings_sum / num_neighbors))\n",
        "    \n",
        "    # Update the graph with the new embeddings\n",
        "    for node, embedding in new_node_embeddings.items():\n",
        "        graph.nodes[node]['embedding'] = embedding\n",
        "    \n",
        "    return graph\n",
        "    \n",
        "# Retrofit the ConceptNet graph to include sentence and label embeddings\n",
        "graph = retrofit_graph(graph, model)\n",
        "\n",
        "# Define the set of possible labels\n",
        "labels = [\"Vaccine\",\"USA\",\"Transportation\",\"News\",\"Mask\",\"China\",\"Advice\"]\n",
        "\n",
        "# Convert the labels to embeddings\n",
        "label_embeddings = []\n",
        "for label in labels:\n",
        "    label_embedding = model.encode(label)\n",
        "    label_embeddings.append(label_embedding)\n",
        "\n",
        "# Loop through each tweet in X\n",
        "for tweet in X:\n",
        "    # Convert the tweet to an embedding\n",
        "    tweet_embedding = model.encode(tweet)\n",
        "    \n",
        "    # Project the tweet and label embeddings to the knowledge graph embedding space\n",
        "    projected_tweet_embedding = torch.matmul(tweet_embedding, P)\n",
        "    projected_label_embeddings = []\n",
        "    for label_embedding in label_embeddings:\n",
        "        projected_label_embedding = torch.matmul(label_embedding, P)\n",
        "        projected_label_embeddings.append(projected_label_embedding)\n",
        "        \n",
        "    # Calculate the cosine similarity between the tweet embedding and each label embedding\n",
        "    similarities = []\n",
        "    for projected_label_embedding in projected_label_embeddings:\n",
        "        similarity = torch.cosine_similarity(projected_tweet_embedding, projected_label_embedding, dim=0)\n",
        "        similarities.append(similarity)\n",
        "        \n",
        "    # Find the label with the highest similarity to the tweet\n",
        "    max_similarity = max(similarities)\n",
        "    max_similarity_index = similarities.index(max_similarity)\n",
        "    predicted_label = labels[max_similarity_index]\n",
        "    \n",
        "    # Output the predicted label for the tweet\n",
        "    print(\"Tweet: \", tweet)\n",
        "    print(\"Predicted label: \", predicted_label)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 728
        },
        "id": "7bcSjTEKrY1Y",
        "outputId": "a80ec3cf-2266-4cca-e4b8-3b9b9821b843"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/huggingface_hub/utils/_errors.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    942\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 943\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/sentence-transformers/sentence-transformers-all-distilroberta-v1/resolve/main/tokenizer_config.json",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    408\u001b[0m         \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m         resolved_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    410\u001b[0m             \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[1;32m   1105\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1106\u001b[0;31m                 metadata = get_hf_file_metadata(\n\u001b[0m\u001b[1;32m   1107\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout)\u001b[0m\n\u001b[1;32m   1440\u001b[0m     )\n\u001b[0;32m-> 1441\u001b[0;31m     \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/huggingface_hub/utils/_errors.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    305\u001b[0m             )\n\u001b[0;32m--> 306\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRepositoryNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRepositoryNotFoundError\u001b[0m: 401 Client Error. (Request ID: Root=1-6400b3af-4ba3091573b3fb26675f2fee)\n\nRepository Not Found for url: https://huggingface.co/sentence-transformers/sentence-transformers-all-distilroberta-v1/resolve/main/tokenizer_config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.\nInvalid username or password.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-8554a5870f64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Load the pre-trained S-BERT model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sentence-transformers/sentence-transformers-all-distilroberta-v1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sentence-transformers/sentence-transformers-all-distilroberta-v1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m         \u001b[0;31m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m         \u001b[0mtokenizer_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_tokenizer_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"_commit_hash\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenizer_config\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mget_tokenizer_config\u001b[0;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, **kwargs)\u001b[0m\n\u001b[1;32m    440\u001b[0m     ```\"\"\"\n\u001b[1;32m    441\u001b[0m     \u001b[0mcommit_hash\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m     resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m    443\u001b[0m         \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m         \u001b[0mTOKENIZER_CONFIG_FILE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mRepositoryNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m         raise EnvironmentError(\n\u001b[0m\u001b[1;32m    425\u001b[0m             \u001b[0;34mf\"{path_or_repo_id} is not a local folder and is not a valid model identifier \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m             \u001b[0;34m\"listed on 'https://huggingface.co/models'\\nIf this is a private repository, make sure to \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: sentence-transformers/sentence-transformers-all-distilroberta-v1 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`."
          ]
        }
      ]
    }
  ]
}